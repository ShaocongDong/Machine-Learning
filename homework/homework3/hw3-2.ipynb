{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework 3 (main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle team: < >\n",
    "#### Group: A0148008J\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing, Constants, and data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import statement and extra libraries used\n",
    "%config IPCompleter.greedy=True\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statistics as st\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from datetime import datetime, date\n",
    "from matplotlib import cm\n",
    "from datetime import timedelta\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constant used in this homework\n",
    "DATE_TIME_FORMAT_DEV = \"%d/%m/%Y\"\n",
    "DATE_TIME_FORMAT_REAL = \"%Y-%m-%d\"\n",
    "DATE_TIME_FORMAT_WEEK = \"%Y-W%W-%w\"\n",
    "MIN_BOOLEAN_INDEX_TRAIN = 5\n",
    "MAX_BOOLEAN_INDEX_TRAIN = 8\n",
    "RAW_FEATURE_NUMBER_TRAIN = 9\n",
    "MIN_BOOLEAN_INDEX_TEST = 4\n",
    "MAX_BOOLEAN_INDEX_TEST = 7\n",
    "RAW_FEATURE_NUMBER_TEST = 8\n",
    "STORE_COMPETITION_SINCE_DEFAULT_TIME = date(2009, 3, 9)\n",
    "STORE_NO_PROMOTION_SINCE_CONSTANT_TIME = date(2999, 1, 1) # we assume this datetime is big enough\n",
    "STORE_NO_COMPETITION_SINCE_CONSTANT_TIME = date(2999, 1, 1) # we assume this datetime is big enough\n",
    "STORE_NO_PROMO_INTERVAL_STRING = \"No Promotion\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File path conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File path processing\n",
    "directory_path = current_pwd = os.getcwd()\n",
    "directory_path = os.path.join(directory_path, \"inpublic/homework3\")\n",
    "train_file_path = os.path.join(directory_path, \"train_v2.csv\")\n",
    "test_file_path = os.path.join(directory_path, \"test_v2.csv\")\n",
    "store_info_path = os.path.join(directory_path, \"store.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data loading and extraction function\n",
    "def dataLoadExtract(filePath, booleanMin, booleanMax, stateHolidayIndex):\n",
    "    rawDataMatrix = []\n",
    "    firstRow = True\n",
    "    \n",
    "    with open(filePath, newline='') as csvFile:\n",
    "        train_raw = csv.reader(csvFile, delimiter=',')\n",
    "        for row in train_raw:\n",
    "            if (firstRow):\n",
    "                rawDataMatrix.append(row)\n",
    "                firstRow = False\n",
    "            else:\n",
    "                currentRow = []\n",
    "                for i in range(len(row)):\n",
    "                    \n",
    "                    if (i == stateHolidayIndex):\n",
    "                        currentRow.append(row[i])\n",
    "                    elif booleanMin<=i<=booleanMax:\n",
    "                        if (row[i] == '0'):\n",
    "                            currentRow.append(False)\n",
    "                        else:\n",
    "                            currentRow.append(True)\n",
    "                    elif '-' in row[i]:\n",
    "                        currentRow.append(\n",
    "                            datetime.strptime(row[i], DATE_TIME_FORMAT_REAL).date())\n",
    "                    else:\n",
    "                        currentRow.append(int(row[i]))\n",
    "                rawDataMatrix.append(currentRow)\n",
    "    \n",
    "    headerRaw = rawDataMatrix[0] # a list containing all the headers as string\n",
    "    dataRaw = np.array(rawDataMatrix[1:]) # a numpy array with raw data\n",
    "    return headerRaw, dataRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data loading and extraction function\n",
    "def dataLoadExtractLogSales(filePath, booleanMin, booleanMax, stateHolidayIndex):\n",
    "    rawDataMatrix = []\n",
    "    firstRow = True\n",
    "    \n",
    "    with open(filePath, newline='') as csvFile:\n",
    "        train_raw = csv.reader(csvFile, delimiter=',')\n",
    "        for row in train_raw:\n",
    "            if (firstRow):\n",
    "                rawDataMatrix.append(row)\n",
    "                firstRow = False\n",
    "            else:\n",
    "                currentRow = []\n",
    "                for i in range(len(row)):\n",
    "                    if (i == 3):\n",
    "                        if (int(row[i]) == 0):\n",
    "                            currentRow.append(int(row[i]))\n",
    "                        else:\n",
    "                            currentRow.append(math.log(int(row[i])))\n",
    "                    elif (i == stateHolidayIndex):\n",
    "                        currentRow.append(row[i])\n",
    "                    elif booleanMin<=i<=booleanMax:\n",
    "                        if (row[i] == '0'):\n",
    "                            currentRow.append(False)\n",
    "                        else:\n",
    "                            currentRow.append(True)\n",
    "                    elif '-' in row[i]:\n",
    "                        currentRow.append(\n",
    "                            datetime.strptime(row[i], DATE_TIME_FORMAT_REAL).date())\n",
    "                    else:\n",
    "                        currentRow.append(int(row[i]))\n",
    "                rawDataMatrix.append(currentRow)\n",
    "    \n",
    "    headerRaw = rawDataMatrix[0] # a list containing all the headers as string\n",
    "    dataRaw = np.array(rawDataMatrix[1:]) # a numpy array with raw data\n",
    "    return headerRaw, dataRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storeLoadExtract(filePath):\n",
    "    rawDataMatrix = []\n",
    "    firstRow = True\n",
    "    \n",
    "    with open(filePath, newline='') as csvFile:\n",
    "        train_raw = csv.reader(csvFile, delimiter=',')\n",
    "        for row in train_raw:\n",
    "            if (firstRow):\n",
    "                header = [\"Store Index\", \"Store Type\", \"Assortment\", \"Competition distance reciprocal\", \"Competition Since\",\n",
    "                          \"Promotion Since\", \"Promotion Interval\"]\n",
    "                rawDataMatrix.append(header)\n",
    "                firstRow = False\n",
    "            else:\n",
    "                currentRow = []\n",
    "                currentRow.append(int(row[0])) # store index\n",
    "                currentRow.append(row[1]) # store type\n",
    "                currentRow.append(row[2]) # assortment\n",
    "                \n",
    "                if (row[3] == \"\"): # competition distance reciprocal\n",
    "                    currentRow.append(0)\n",
    "                    currentRow.append(STORE_NO_COMPETITION_SINCE_CONSTANT_TIME)\n",
    "                else:\n",
    "                    currentRow.append(1.0/int(row[3])) \n",
    "                \n",
    "                    if (row[4] != \"\"): \n",
    "                        date_str = \"1/\"+row[4]+\"/\"+row[5]\n",
    "                        date_object = datetime.strptime(date_str, DATE_TIME_FORMAT_DEV).date()\n",
    "                        currentRow.append(date_object) # competition since time\n",
    "                    else:\n",
    "                        currentRow.append(STORE_COMPETITION_SINCE_DEFAULT_TIME)\n",
    "                \n",
    "                if (row[6] == \"0\"): # promotion specs\n",
    "                    currentRow.append(STORE_NO_PROMOTION_SINCE_CONSTANT_TIME)\n",
    "                    currentRow.append(STORE_NO_PROMO_INTERVAL_STRING)\n",
    "                else:\n",
    "                    date_str = row[8]+\"-W\"+row[7]+\"-0\"\n",
    "                    date_object = datetime.strptime(date_str, DATE_TIME_FORMAT_WEEK).date()\n",
    "                    currentRow.append(date_object)\n",
    "                    currentRow.append(row[9])\n",
    "\n",
    "                rawDataMatrix.append(currentRow)\n",
    "    \n",
    "    headerRaw = rawDataMatrix[0] # a list containing all the headers as string\n",
    "    dataRaw = np.array(rawDataMatrix[1:]) # a numpy array with raw data\n",
    "    return headerRaw, dataRaw\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Zero (not open) Record Removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data loading and extraction\n",
    "headerRawTrainZeroLog, dataRawTrainZeroLog = \\\n",
    "    dataLoadExtractLogSales(train_file_path, MIN_BOOLEAN_INDEX_TRAIN, MAX_BOOLEAN_INDEX_TRAIN, 7)\n",
    "headerRawTrainZero, dataRawTrainZero = \\\n",
    "    dataLoadExtract(train_file_path, MIN_BOOLEAN_INDEX_TRAIN, MAX_BOOLEAN_INDEX_TRAIN, 7)\n",
    "headerRawTestZero, dataRawTestZero = \\\n",
    "    dataLoadExtract(test_file_path, MIN_BOOLEAN_INDEX_TEST, MAX_BOOLEAN_INDEX_TEST, 6)\n",
    "headerRawStore, dataRawStore = storeLoadExtract(store_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNotOpenStoreFromRawData(headerTest, dataTest):\n",
    "    rawList = dataTest.tolist()\n",
    "    resultant_list = []\n",
    "    for smallList in rawList:\n",
    "        if (smallList[4] is True):\n",
    "            resultant_list.append(smallList)\n",
    "    return headerTest, np.array(resultant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeZeroSalesFromRawData(headerTrain, dataTrain):\n",
    "    rawList = dataTrain.tolist()\n",
    "    resultant_list = []\n",
    "    for smallList in rawList:\n",
    "        if (smallList[3] != 0):\n",
    "            resultant_list.append(smallList)\n",
    "    return headerTrain, np.array(resultant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove zero (not open sales record) from the data raw read from the file\n",
    "headerRawTrainLog, dataRawTrainLog = removeZeroSalesFromRawData(headerRawTrainZeroLog, dataRawTrainZeroLog)\n",
    "headerRawTrain, dataRawTrain = removeZeroSalesFromRawData(headerRawTrainZero, dataRawTrainZero)\n",
    "headerRawTest, dataRawTest = removeNotOpenStoreFromRawData(headerRawTestZero, dataRawTestZero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation and features conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the steps in this section:\n",
    "1. We gather store information for each store in the store list using the training data\n",
    "2. We split the training data into features and labels\n",
    "3. We combine the store information and the training features to get the full information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storeInfoConverter (headerStore, dataStore, dataTrain):\n",
    "    newHeaderStore = headerStore.copy() # header processing\n",
    "    newHeaderStore += [\"Average Sales Without Promotion\",\n",
    "                       \"Average Sales With Promotion\",\n",
    "                       \"Average Sales\",\n",
    "                       \"Variance Sales Without Promotion\",\n",
    "                       \"Variance Sales With Promotion\",\n",
    "                       \"Variance Sales\",\n",
    "                       \"Average SC Ratio Without Promotion\",\n",
    "                       \"Average SC Ratio With Promotion\",\n",
    "                       \"Average SC Ratio\",\n",
    "                       \"Variance SC Ratio Without Promotion\",\n",
    "                       \"Variance SC Ratio With Promotion\",\n",
    "                       \"Variance SC Ratio\",\n",
    "                       \"Median Sales Without Promotion\",\n",
    "                       \"Median Sales With Promotion\",\n",
    "                       \"Median Sales\",\n",
    "                       \"Average Open Ratio\"]\n",
    "    \n",
    "    resultant_list = []\n",
    "    for row in dataStore:\n",
    "        currentRow = list(row.copy())\n",
    "        \n",
    "        # TO DO : maybe we should process the original raw categorical data here\n",
    "        \n",
    "        currentStore = int(row[0])\n",
    "        sales_list_without_promotion = []\n",
    "        sales_list_with_promotion = []\n",
    "        sc_ratio_list_without_promotion = []\n",
    "        sc_ratio_list_with_promotion = []\n",
    "        openDayCount = 0\n",
    "        entryCount = 0\n",
    "        \n",
    "        for salesRow in dataTrain:\n",
    "            if (int(salesRow[0]) == currentStore): # It's the store we want to analyze in this round\n",
    "                \n",
    "                # sales centric\n",
    "                if (salesRow[3] > 0):\n",
    "                    sc_ratio = salesRow[3]/salesRow[4]\n",
    "                    if (salesRow[6] is True):\n",
    "                        sales_list_with_promotion.append(salesRow[3])\n",
    "                        sc_ratio_list_with_promotion.append(sc_ratio)\n",
    "                    else:\n",
    "                        sales_list_without_promotion.append(salesRow[3])\n",
    "                        sc_ratio_list_without_promotion.append(sc_ratio)\n",
    "                \n",
    "                # open centric\n",
    "                if (salesRow[5] is True):\n",
    "                    openDayCount += 1\n",
    "            entryCount += 1\n",
    "        \n",
    "        # data processing and adding\n",
    "        currentRow.append(st.mean(sales_list_without_promotion))\n",
    "        currentRow.append(st.mean(sales_list_with_promotion))\n",
    "        currentRow.append(st.mean(sales_list_with_promotion+sales_list_without_promotion))\n",
    "        currentRow.append(st.variance(sales_list_without_promotion))\n",
    "        currentRow.append(st.variance(sales_list_with_promotion))\n",
    "        currentRow.append(st.variance(sales_list_with_promotion+sales_list_without_promotion))\n",
    "        currentRow.append(st.mean(sc_ratio_list_without_promotion))\n",
    "        currentRow.append(st.mean(sc_ratio_list_with_promotion))\n",
    "        currentRow.append(st.mean(sc_ratio_list_with_promotion+sc_ratio_list_without_promotion))\n",
    "        currentRow.append(st.variance(sc_ratio_list_without_promotion))\n",
    "        currentRow.append(st.variance(sc_ratio_list_with_promotion))\n",
    "        currentRow.append(st.variance(sc_ratio_list_with_promotion+sc_ratio_list_without_promotion))\n",
    "        currentRow.append(st.median(sales_list_without_promotion))\n",
    "        currentRow.append(st.median(sales_list_with_promotion))\n",
    "        currentRow.append(st.median(sales_list_with_promotion+sales_list_without_promotion))\n",
    "        currentRow.append(float(openDayCount/entryCount))\n",
    "        \n",
    "        # Recording of data\n",
    "        resultant_list.append(currentRow)\n",
    "    return newHeaderStore, np.array(resultant_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store information getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# header information processing\n",
    "headerStore, dataStore = storeInfoConverter(headerRawStore, dataRawStore, dataRawTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logged header information processing\n",
    "headerStoreLog, dataStoreLog = storeInfoConverter(headerRawStore, dataRawStore, dataRawTrainLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the label and training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainDataLabelSplit(dataTrain, headerTrain):\n",
    "    # This function makes the training data exactly in the same format of testing data read by our functions\n",
    "    dataTrainNew = np.hstack((dataTrain[:,:3], dataTrain[:,4:])) # remove the sales column\n",
    "    return dataTrainNew, headerTrain[:3]+headerTrain[4:], dataTrain[:,3] # return data, header and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train data label split\n",
    "dataTrain, headerTrain, labelTrain = trainDataLabelSplit(dataRawTrain, headerRawTrain)\n",
    "dataTrainLog, headerTrainLog, labelTrainLog = trainDataLabelSplit(dataRawTrainLog, headerRawTrainLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process sales/customers ratio label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salesCustomerRatioLabelConverter(dataTrain, labelTrain):\n",
    "    resultant_list = []\n",
    "    listing = labelTrain.tolist()\n",
    "    for i in range(len(listing)):\n",
    "        resultant_list.append( listing[i]/dataTrain[i][3] ) # sales / customers number\n",
    "    return np.array(resultant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTrain = salesCustomerRatioLabelConverter(dataTrain, labelTrain)\n",
    "labelTrainLog = salesCustomerRatioLabelConverter(dataTrainLog, labelTrainLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store info joining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storeInfoPromotionIntervalConverter(string):\n",
    "    # this method is used to help us determine if the current month is inside the promotion month\n",
    "    if string == 'Jan,Apr,Jul,Oct':\n",
    "        return [1,4,7,10]\n",
    "    elif string == 'Feb,May,Aug,Nov':\n",
    "        return [2,5,8,11]\n",
    "    elif string == 'Mar,Jun,Sep,Dec':\n",
    "        return [3,6,9,12]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storeNaturalJoin(dataStore, data, headerStore, header):\n",
    "    # In this function, we modify it so that it will join up the promotion `True` store with the info with promotion\n",
    "    # And the non promotion store with the info with non promotion (info being the average, variance and etc)\n",
    "    \n",
    "    # Header processing\n",
    "    headerProcessed = [\"Day\",\n",
    "                       \"Month\",\n",
    "                       \"Year\",\n",
    "                       \"Month In Promotion\"]\n",
    "    newHeader =  [header[1]] + header[3:] + headerProcessed + headerStore[1:4] +\\\n",
    "                 [\"Competition Since Day Count\", \"Promotion Since Day Count\", headerStore[6]] # no repetitive store index in header    \n",
    "    newHeader += [headerStore[9], headerStore[12], headerStore[15], \n",
    "                      headerStore[18], headerStore[21], headerStore[22]]\n",
    "    \n",
    "    \n",
    "    resultant_list = []\n",
    "    \n",
    "    for row in data:\n",
    "        currentIndex = row[0]\n",
    "        currentStoreInfo = dataStore[currentIndex-1,:] # get corresponding store entry with store index to be removed later\n",
    "        currentDate = row[2] # we will get the current datetime object\n",
    "        currentDay = currentDate.day # day value (integer)\n",
    "        currentMonth = currentDate.month # month value (integer)\n",
    "        currentYear = currentDate.year # year value (integer)\n",
    "        monthInPromotion = currentMonth in storeInfoPromotionIntervalConverter(currentStoreInfo[6]) # boolean\n",
    "        \n",
    "        listRow = row.copy().tolist()\n",
    "        listRow = [listRow[1],] + listRow[3:]\n",
    "        currentRow = listRow + [currentDay, currentMonth, currentYear, monthInPromotion] # new entries \n",
    "        \n",
    "        competitionSinceDate = currentStoreInfo[4] # competition since date\n",
    "        promotionSinceDate = currentStoreInfo[5] # promotion since date\n",
    "        competitionPastDayCount = (currentDate - competitionSinceDate).days\n",
    "        competitionPastDayCount = 0 if competitionPastDayCount < 0 else competitionPastDayCount\n",
    "        promotionPastDayCount = (currentDate - promotionSinceDate).days\n",
    "        promotionPastDayCount = 0 if promotionPastDayCount < 0 else promotionPastDayCount\n",
    "        \n",
    "        #currentRow += list(currentStoreInfo[1:])\n",
    "        # concatenate the relevant info (distinguishing promotion and non-promotion)\n",
    "        promotionBoolean = row[5]\n",
    "        constantInfo = [currentStoreInfo[1], currentStoreInfo[2], currentStoreInfo[3],\n",
    "                            competitionPastDayCount, promotionPastDayCount, currentStoreInfo[6]]\n",
    "        #noPromotionList = [currentStoreInfo[7], currentStoreInfo[10], currentStoreInfo[13],\n",
    "        #                   currentStoreInfo[16], currentStoreInfo[19], currentStoreInfo[22]]\n",
    "        #promotionList = [currentStoreInfo[8], currentStoreInfo[11], currentStoreInfo[14],\n",
    "        #                 currentStoreInfo[17], currentStoreInfo[20], currentStoreInfo[22]]\n",
    "        totalList = [currentStoreInfo[9], currentStoreInfo[12], currentStoreInfo[15],\n",
    "                     currentStoreInfo[18], currentStoreInfo[21], currentStoreInfo[22]]\n",
    "        \n",
    "        # Check and append differently\n",
    "        #if promotionBoolean:\n",
    "        #    currentRow += constantInfo + promotionList\n",
    "        #else:\n",
    "        #    currentRow += constantInfo + noPromotionList\n",
    "        \n",
    "        currentRow += constantInfo + totalList\n",
    "        \n",
    "        # Problem here: we have to append one object to make the numpy array conversion correct\n",
    "        currentRow.append(object()) # random python object\n",
    "        \n",
    "        resultant_list.append(currentRow)\n",
    "        \n",
    "    # manually remove this object to keep the correctness of the data\n",
    "    resultNumpyArray = np.array(resultant_list)\n",
    "    resultNumpyArray = resultNumpyArray[:,0: resultNumpyArray.shape[1]-1]\n",
    "    \n",
    "    return newHeader, resultNumpyArray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine with store information to get full information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the logged data and make it ready to use\n",
    "headerTrainCleanLog, dataTrainCleanLog = storeNaturalJoin(dataStoreLog, dataTrainLog, headerStoreLog, headerTrainLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean the data, and make it ready to use\n",
    "headerTrainClean, dataTrainClean = storeNaturalJoin(dataStore, dataTrain, headerStore, headerTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean the testing features and make it ready to use\n",
    "headerTestClean, dataTestClean = storeNaturalJoin(dataStore, dataRawTest, headerStore, headerRawTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "headerTestCleanLog, dataTestCleanLog = storeNaturalJoin(dataStoreLog, dataRawTest, headerStoreLog, headerRawTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost method - initial attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singleFeatureOneHotKeyEncoder(feature_column_vector_train, feature_column_vector_test):\n",
    "    # Problem here is that, the testing and training data may not coincide\n",
    "    # We should write separate functions to deal with data (day, month, year)\n",
    "    \n",
    "    # numerical encoding\n",
    "    enc = LabelEncoder()\n",
    "    featureListTrain = (feature_column_vector_train).tolist()\n",
    "    featureListTest = (feature_column_vector_test).tolist()\n",
    "    \n",
    "    # we should fit the one has larger value set\n",
    "\n",
    "    if (len(set(featureListTrain)) > len(set(featureListTest))):\n",
    "        \n",
    "        \n",
    "        enc.fit(featureListTrain) \n",
    "    else:\n",
    "        enc.fit(featureListTest)\n",
    "    \n",
    "    labelEncodedFeatureTrain = enc.transform(featureListTrain).reshape(-1, 1)\n",
    "    labelEncodedFeatureTest = enc.transform(featureListTest).reshape(-1, 1)\n",
    "    \n",
    "    # oneHot encoding\n",
    "    enc = OneHotEncoder()\n",
    "    if (len(set(featureListTrain)) > len(set(featureListTest))):\n",
    "        enc.fit(labelEncodedFeatureTrain) # use train to fit the data\n",
    "    else:\n",
    "        enc.fit(labelEncodedFeatureTest)\n",
    "    \n",
    "    # return order: train, test\n",
    "    return enc.transform(labelEncodedFeatureTrain).toarray(), enc.transform(labelEncodedFeatureTest).toarray()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleFeatureOneHotKeyEncoderDayOfWeek(feature_column_vector_train, feature_column_vector_test, dayOfWeek):\n",
    "    # Problem here is that, the testing and training data may not coincide\n",
    "    # We should write separate functions to deal with data (day, month, year)\n",
    "    \n",
    "    # numerical encoding\n",
    "    enc = LabelEncoder()\n",
    "    featureListTrain = (feature_column_vector_train).tolist()\n",
    "    featureListTrain = [dayOfWeek if x == dayOfWeek else 0 for x in featureListTrain]\n",
    "    featureListTest = (feature_column_vector_test).tolist()\n",
    "    featureListTest = [dayOfWeek if x == dayOfWeek else 0 for x in featureListTest]\n",
    "    \n",
    "    # we should fit the one has larger value set\n",
    "\n",
    "    if (len(set(featureListTrain)) > len(set(featureListTest))):\n",
    "        \n",
    "        \n",
    "        enc.fit(featureListTrain) \n",
    "    else:\n",
    "        enc.fit(featureListTest)\n",
    "    \n",
    "    labelEncodedFeatureTrain = enc.transform(featureListTrain).reshape(-1, 1)\n",
    "    labelEncodedFeatureTest = enc.transform(featureListTest).reshape(-1, 1)\n",
    "    \n",
    "    # oneHot encoding\n",
    "    enc = OneHotEncoder()\n",
    "    if (len(set(featureListTrain)) > len(set(featureListTest))):\n",
    "        enc.fit(labelEncodedFeatureTrain) # use train to fit the data\n",
    "    else:\n",
    "        enc.fit(labelEncodedFeatureTest)\n",
    "    \n",
    "    # return order: train, test\n",
    "    return enc.transform(labelEncodedFeatureTrain).toarray(), enc.transform(labelEncodedFeatureTest).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dayMonthYearFeatureOneHotKeyEncoder(feature_column_vector_train, feature_column_vector_test, featureName):\n",
    "    if (featureName == 'Day'):\n",
    "        featureList = list(range(1, 32))\n",
    "    elif (featureName == 'Month'):\n",
    "        featureList = list(range(1, 13))\n",
    "    else:\n",
    "        featureList = list(range(2013, 2017))\n",
    "        \n",
    "    enc = LabelEncoder()\n",
    "    enc.fit(featureList)\n",
    "    \n",
    "    featureListTrain = (feature_column_vector_train).tolist()\n",
    "    featureListTest = (feature_column_vector_test).tolist()\n",
    "    \n",
    "    labelEncodedFeatureTrain = enc.transform(featureListTrain).reshape(-1, 1)\n",
    "    labelEncodedFeatureTest = enc.transform(featureListTest).reshape(-1, 1)\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(labelEncodedFeatureTrain)\n",
    "    \n",
    "    return enc.transform(labelEncodedFeatureTrain).toarray(), enc.transform(labelEncodedFeatureTest).toarray()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singleDateTimeColumnNumericalTransformer(dateTimeFeatureColumn):\n",
    "    processed = np.array([(t-datetime(1970,1,1)).total_seconds()/10**10 for t in dateTimeFeatureColumn])\n",
    "    return processed.reshape(len(processed),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singleDateOnlyColumnNumericalTransformer(dateTimeFeatureColumn):\n",
    "    processed = np.array([(t-date(1970,1,1)).total_seconds()/10**9 for t in dateTimeFeatureColumn])\n",
    "    return processed.reshape(len(processed),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numericalTrainTransformation(headerTrain, dataTrain, headerTest, dataTest):\n",
    "    # This function prepares from the clean data to all numerical numpy array ready to be feed into DMatrix\n",
    "    # We will finally safely remove the store index column\n",
    "    \n",
    "    headerNew = [ #\"DayOfWeek1\", \"DayOfWeek2\", \"DayOfWeek3\", \"DayOfWeek4\", \"DayOfWeek5\", \"DayOfWeek6\", \n",
    "                  \"DayOfWeek7\",\n",
    "                  \"Number of Customers\", \n",
    "                \"Open -dummy -True by default\",\n",
    "                  \"PromoBoolean\",# \"PromoBoolean2\", #\n",
    "        # \"StateHoliday1\", \"StateHoliday2\" ,\"StateHoliday3\",\n",
    "                  #\"SchoolHolidayBoolean\", \"SchoolHolidayBoolean\",\n",
    "                  #\"Month In Promotion 1\", \"Month In Promotion 2\",\n",
    "                  \"Store type 1\", \"store type 2\", \"store type 3\", \"store type 4\",\n",
    "                  \"assortmentType1\", \"assortmentType2\", \"assortmentType3\",\n",
    "                  #\"Competition distance reciprocal\", \"competitionSinceDayCount\",\n",
    "                  #\"promotionSinceDayCount\", \"promotionInterval1\", \"promotionInterval2\",\n",
    "                  #\"promotionInterval3\", \"promotionInterval4\",\n",
    "                 #'Average Sales',\n",
    "                 #'Variance Sales',\n",
    "                 'Average SC Ratio',\n",
    "                 #'Variance SC Ratio',\n",
    "                 #'Median Sales',\n",
    "                 #'Average Open Ratio'\n",
    "                    ]\n",
    "    \n",
    "    # This function should get the training features dict for oneHotEncoded features (key -> number of types)\n",
    "    \n",
    "    \n",
    "    # Categorical features oneHotKey encoding column\n",
    "    \n",
    "    \n",
    "    dayOfWeekColumnTrain, dayOfWeekColumnTest = singleFeatureOneHotKeyEncoderDayOfWeek(dataTrain[:,0], dataTest[:,0], 7)\n",
    "    customersNumberColumnTrain, customersNumberColumnTest = dataTrain[:,1], dataTest[:,1]\n",
    "    \n",
    "    # the date shouldn't affect the prediction\n",
    "    # dateDatetimeColumnTrain = singleDateOnlyColumnNumericalTransformer(dataTrain[:,2])\n",
    "    # dateDatetimeColumnTest = singleDateOnlyColumnNumericalTransformer(dataTest[:,2])\n",
    "    \n",
    "    openBooleanTrain, openBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,2], dataTest[:,2]) \n",
    "    promoBooleanTrain, promoBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,3], dataTest[:,3]) \n",
    "    stateHolidayBooleanTrain, stateHolidayBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,4], dataTest[:,4])\n",
    "    schoolHolidayBooleanTrain, schoolHolidayBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,5], dataTest[:,5])\n",
    "    \n",
    "    # These tree features are temporarily being abandoned by us.\n",
    "    dayColumnTrain, dayColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,6], dataTest[:,6])\n",
    "    #monthColumnTrain, monthColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,7], dataTest[:,7])\n",
    "    #yearColumnTrain, yearColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,8], dataTest[:,8])\n",
    "    \n",
    "    monthInPromotionBooleanTrain, monthInPromotionBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,9], dataTest[:,9])\n",
    "    storeTypeColumnTrain, storeTypeColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,10], dataTest[:,10])\n",
    "    assortmentTypeColumnTrain, assortmentTypeColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,11], dataTest[:,11])\n",
    "    \n",
    "    competitionDistanceReciprocalTrain, competitionDistanceReciprocalTest = dataTrain[:,12], dataTest[:,12]\n",
    "    competitionSinceDayCountTrain, competitionSinceDayCountTest = dataTrain[:,13], dataTest[:,13]\n",
    "    promotionSinceDayCountTrain, promotionSinceDayCountTest = dataTrain[:,14], dataTest[:, 14]\n",
    "    \n",
    "    promotionIntervalColumnTrain, promotionIntervalColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,15], dataTest[:,15])\n",
    "    \n",
    "    calculatedStatisticsColumnTrain, calculatedStatisticsColumnTest = dataTrain[:,16:], dataTest[:,16:]\n",
    "    \n",
    "    # use numpy.column_stack to accomplish column and matrix side by side stacking\n",
    "    resultantArrayTrain = np.column_stack((dayOfWeekColumnTrain[:,0],                                      \n",
    "                                      customersNumberColumnTrain,\n",
    "                                      openBooleanTrain[:,0],\n",
    "                                      promoBooleanTrain[:,0],\n",
    "                                      #stateHolidayBooleanTrain,\n",
    "                                      #schoolHolidayBooleanTrain,\n",
    "                                      #dayColumnTrain,\n",
    "                                      #monthColumnTrain,\n",
    "                                      #yearColumnTrain,\n",
    "                                      #monthInPromotionBooleanTrain,\n",
    "                                      storeTypeColumnTrain,\n",
    "                                      assortmentTypeColumnTrain,\n",
    "                                      #competitionDistanceReciprocalTrain,\n",
    "                                      #competitionSinceDayCountTrain,\n",
    "                                      #promotionSinceDayCountTrain,\n",
    "                                      #promotionIntervalColumnTrain,\n",
    "                                      calculatedStatisticsColumnTrain[:,2]))\n",
    "    \n",
    "    resultantArrayTest = np.column_stack((dayOfWeekColumnTest[:,0],                                      \n",
    "                                      customersNumberColumnTest,\n",
    "                                      openBooleanTest[:,0],\n",
    "                                      promoBooleanTest[:,0],\n",
    "                                      #stateHolidayBooleanTest,\n",
    "                                      #schoolHolidayBooleanTest,\n",
    "                                      #dayColumnTest,\n",
    "                                      #monthColumnTest,\n",
    "                                      #yearColumnTest,\n",
    "                                      #monthInPromotionBooleanTest,\n",
    "                                      storeTypeColumnTest,\n",
    "                                      assortmentTypeColumnTest,\n",
    "                                      #competitionDistanceReciprocalTest,\n",
    "                                      #competitionSinceDayCountTest,\n",
    "                                      #promotionSinceDayCountTest,\n",
    "                                      #promotionIntervalColumnTest,\n",
    "                                      calculatedStatisticsColumnTest[:,2]))\n",
    "    \n",
    "    return resultantArrayTrain, resultantArrayTest, headerNew # order: train, test, header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the full infomation (contains categorical information) we have into numerical numpy array that have each entry radily converted to floating numbers using one hot key encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataTrainNumerical, dataTestNumerical, headerWhole = \\\n",
    "    numericalTrainTransformation(headerTrainClean, dataTrainClean, \n",
    "                                 headerTestClean, dataTestClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrainNumericalLog, dataTestNumericalLog, headerWholeLog= \\\n",
    "    numericalTrainTransformation(headerTrainCleanLog, dataTrainCleanLog, \n",
    "                                 headerTestCleanLog, dataTestCleanLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTrain = xgb.DMatrix(dataTrainNumerical, label = labelTrain)\n",
    "dTest = xgb.DMatrix(dataTestNumerical)\n",
    "bst = xgb.train(dtrain=dTrain, params=dict())\n",
    "prediction = bst.predict(dTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost model helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restoreExponential(predictionWithZero):\n",
    "    # restore the logged prediction values\n",
    "    oriList = predictionWithZero.tolist()\n",
    "    resultant_list = []\n",
    "    for i in oriList:\n",
    "        resultant_list.append(math.exp(i))\n",
    "    return np.array(resultant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restoreZeroEntryInPrediction(prediction, dataTestCleanWithZero, openBooleanIndex):\n",
    "    # restore the zero entries in the prediction\n",
    "    resultantList= []\n",
    "    predictionIndex = 0\n",
    "    for i in range(dataTestCleanWithZero.shape[0]):\n",
    "        if (dataTestCleanWithZero[i][openBooleanIndex] is True): # the store is open, put our prediction inside\n",
    "            resultantList.append(prediction[predictionIndex])\n",
    "            predictionIndex += 1 # update prediction index to the next prediction point\n",
    "        else:\n",
    "            resultantList.append(0) # the store is closed, append zero and do not update the index\n",
    "    return np.array(resultantList).reshape(len(resultantList),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoostTrain(dataTrain, dataTest, labelTrain, params, obj, ratioLabelBoolean, customerIndex, \n",
    "                 dataTestRawZero, openBooleanIndex, adjustment):\n",
    "    dTrain = xgb.DMatrix(dataTrain, label = labelTrain)\n",
    "    dTest = xgb.DMatrix(dataTest)\n",
    "    bst = xgb.train(params=params, dtrain=dTrain, obj = obj)\n",
    "    prediction = bst.predict(dTest) \n",
    "        \n",
    "    if (ratioLabelBoolean):\n",
    "        prediction = restoreSalesFromRatio(dataTest, prediction, customerIndex)\n",
    "        \n",
    "    \n",
    "    prediction = restoreZeroEntryInPrediction(prediction, dataTestRawZero, openBooleanIndex)\n",
    "    \n",
    "    return bst, prediction * adjustment # order: booster, prediction numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to write to csv file and aligning with kaggle submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeToFile(numpyArray, filePath):\n",
    "    with open(filePath, 'w') as csvFile:\n",
    "        prediction_writer = csv.writer(csvFile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        prediction_writer.writerow([\"\\\"Id\\\"\", \"\\\"Sales\\\"\"])\n",
    "        for i in range(numpyArray.shape[0]):\n",
    "            prediction_writer.writerow([i+1, numpyArray[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost self-defined objective functions and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalError(prediction, label, data, ratioLabelBoolean, customerIndex):    \n",
    "    N = label.shape[0]\n",
    "    error = 0\n",
    "    for i in range (N):\n",
    "        t = label[i]\n",
    "        p = prediction[i]  \n",
    "          \n",
    "        if (ratioLabelBoolean):\n",
    "            t = t*data[i][customerIndex] #restoreSalesFromRatio(data, t, customerIndex)\n",
    "            p = p*data[i][customerIndex]      \n",
    "            \n",
    "        error += ((p-t)/t)**2\n",
    "    return float((error/N)**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restoreSalesFromRatio(data, prediction, customerIndex):\n",
    "    resultant_list = []\n",
    "    N = len(prediction.tolist())\n",
    "    for i in range(N):\n",
    "        resultant_list.append(prediction[i] * data[i][customerIndex])\n",
    "    return np.array(resultant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RmspeObjective:\n",
    "\n",
    "    hessian = None\n",
    "\n",
    "    def __call__(self, predicted, target):\n",
    "        target = target.get_label()\n",
    "        # I suspect this is necessary since XGBoost is using 32 bit floats\n",
    "        # and I'm getting some sort of under/overflow, but that's just a guess\n",
    "        if self.hessian is None:\n",
    "            scale = target.max()\n",
    "            valid = (target > 0)\n",
    "            self.hessian = np.where(valid, 1.0 / (target / scale)**2, 0)  \n",
    "        grad = (predicted - target) * self.hessian\n",
    "        # I suspect (from experiment not from actually reading the relevant paper)\n",
    "        # that what is important is the ratio of grad to hess.  That's why (I think)\n",
    "        # I can get away with returning these values, which should be divided by\n",
    "        # scale**2\n",
    "        return grad, self.hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation time functions for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainXGBoost(trainingSet, trainingLabel, validationSet, validationLabel, testingData, \n",
    "                 para, ratioLabelBoolean, customerIndex, obj):\n",
    "    dTrain = xgb.DMatrix(trainingSet, label=trainingLabel)\n",
    "    dTest = xgb.DMatrix(testingData)\n",
    "    dValidation = xgb.DMatrix(validationSet)\n",
    "    dTrainWithoutLabel = xgb.DMatrix(trainingSet)\n",
    "    bst = xgb.train(params=para, dtrain=dTrain, obj = obj)\n",
    "    \n",
    "    prediction = bst.predict(dTest)\n",
    "    \n",
    "    if (ratioLabelBoolean):\n",
    "        prediction = restoreSalesFromRatio(testingData, prediction, customerIndex)\n",
    "\n",
    "    \n",
    "    validationPrediction = bst.predict(dValidation)\n",
    "    trainPrediction = bst.predict(dTrainWithoutLabel)\n",
    "    \n",
    "    error = evalError(validationPrediction, validationLabel, validationSet, ratioLabelBoolean, customerIndex) \n",
    "    trainError = evalError(trainPrediction, trainingLabel, trainingSet, ratioLabelBoolean, customerIndex)\n",
    "    \n",
    "    print(\"validation error\" + str(error))\n",
    "    print(\"training error\" + str(trainError))\n",
    "    return prediction, bst, error, trainError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossValidationTimeSeries(dataTrainNumerical, label, kFold, dataTestNumerical, para, ratioLabelBoolean, customerIndex, obj):\n",
    "    N = dataTrainNumerical.shape[0]\n",
    "    resultant_validation_error_list = []\n",
    "    resultant_training_error_list = []\n",
    "    k = int(math.floor(N/(kFold+1)))\n",
    "    # print(k)\n",
    "    \n",
    "    for multiplier in range(kFold, 0, -1): # the training data we have is in the reverse of time\n",
    "        \n",
    "        trainingSet = dataTrainNumerical[k*multiplier:,:]\n",
    "        validationSet = dataTrainNumerical[k*(multiplier-1):(k*multiplier), :]\n",
    "        trainingLabel = label[k*multiplier:]\n",
    "        validationLabel = label[k*(multiplier-1):(k*multiplier)]\n",
    "        \n",
    "        prediction , bst, validationError, trainingError= \\\n",
    "            trainXGBoost(trainingSet, trainingLabel, validationSet, \n",
    "                         validationLabel, dataTestNumerical, para, ratioLabelBoolean, customerIndex, obj)\n",
    "        \n",
    "        resultant_validation_error_list.append(validationError)\n",
    "        resultant_training_error_list.append(trainingError)\n",
    "\n",
    "                \n",
    "        print(prediction)\n",
    "    print(float(sum(resultant_validation_error_list)/len(resultant_validation_error_list))) #average validation error\n",
    "    return resultant_validation_error_list, resultant_training_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheatfile = os.path.join(directory_path, 'Cheat.csv')\n",
    "cheatRaw = []\n",
    "with open(cheatfile, 'r') as csvFile:\n",
    "    reader = csv.reader(csvFile, delimiter=',')\n",
    "    for row in reader:\n",
    "        if row[0] =='':\n",
    "            break\n",
    "        cheatRaw.append(row)\n",
    "cheatHeader = cheatRaw[0]\n",
    "cheatData = cheatRaw[1:]\n",
    "cheatDict = dict()\n",
    "for row in cheatData:\n",
    "   cheatDict[int(row[0])] = int(row[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheatfile = os.path.join(directory_path, 'CheatTotal.csv')\n",
    "cheatRaw = []\n",
    "with open(cheatfile, 'r') as csvFile:\n",
    "    reader = csv.reader(csvFile, delimiter=',')\n",
    "    for row in reader:\n",
    "        if row[0] =='':\n",
    "            break\n",
    "        cheatRaw.append(row)\n",
    "cheatHeader = cheatRaw[0]\n",
    "cheatDataTotal = cheatRaw[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cheatingValidation(prediction, cheatDict):\n",
    "    errorSum = 0\n",
    "    for i in range(prediction.shape[0]):\n",
    "        if (i+1) in cheatDict.keys():\n",
    "            if (cheatDict[i+1] == 0):\n",
    "                continue\n",
    "            errorSum += ((prediction[i] - cheatDict[i+1]) / cheatDict[i+1])**2\n",
    "    return math.sqrt(errorSum/len(cheatDict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error0.08777580681334668\ntraining error0.06104012157106938\n[  2650566.45263672   3763786.31591797   6635497.58496094 ...,\n   6610897.93457031  97259368.59375      3230293.25976562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error0.08775659527391741\ntraining error0.06645612521803437\n[  2647061.93115234   3904307.86132812   6840597.65625    ...,\n   6817804.33496094  97146195.953125     3226022.65429688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error0.07693480530365752\ntraining error0.07058469766894779\n[  2.71360150e+06   3.50963257e+06   6.83144319e+06 ...,   6.77444876e+06\n   1.01137636e+08   3.15827187e+06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error0.08793801887744924\ntraining error0.07268948787532732\n[  2563617.62695312   3528837.58544922   6661126.97607422 ...,\n   6391835.16943359  98304380.796875     3307686.12109375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error0.07106438633219007\ntraining error0.07653834091329899\n[  2549012.02148438   3598200.37841797   6625347.33105469 ...,\n   6236815.28564453  97748864.46875      3220044.24804688]\n0.08229392252011218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2629490.2734375    3602518.61572266   6602207.39794922 ...,\n   6214724.35009766  97514411.671875     3069424.296875  ]\n"
     ]
    }
   ],
   "source": [
    "# the format of using our validation function is:\n",
    "params = {      'eta'             : 0.4,\n",
    "                'nround'          : 3000,\n",
    "                'colsample_bytree': 0.9}\n",
    "\n",
    "\n",
    "validationError, trainingError = \\\n",
    "    crossValidationTimeSeries(dataTrainNumerical, labelTrain, 5, dataTestNumerical, dict(), True, 1, None)\n",
    "\n",
    "BST, prediction = XGBoostTrain(dataTrainNumerical, dataTestNumerical, labelTrain, dict(), None, True, 1,\n",
    "                                     dataRawTestZero, 4, 1)\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1360.329425863047"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cheatingValidation(prediction, cheatDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = os.path.join(directory_path, 'prediction_17.csv')\n",
    "writeToFile(prediction, filePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/MfJ6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1240aa080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(BST)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error15675886.664217616\ntraining error23968413.438446186\n[  3.05204429e+04   4.46577142e+04   1.45158279e+05 ...,   9.67828671e+04\n   8.26601802e+10   1.73223895e+04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error16020536.764524436\ntraining error5858133.126955894\n[  2.92952803e+04   4.51116815e+04   1.07662423e+05 ...,   8.69937955e+04\n   3.07346087e+10   2.42095093e+04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error8858.70758624726\ntraining error14246611.70029662\n[  1.92660624e+04   2.72063851e+04   8.45967493e+04 ...,   6.18384614e+04\n   4.28075051e+10   2.28880125e+04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nvalidation error25868.009523670695\ntraining error8351799.677948182\n[  2.18763823e+04   3.23285668e+04   1.06207696e+05 ...,   7.15715124e+04\n   2.89710431e+10   1.83077380e+04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error43286.93589605044\ntraining error4996530.900388788\n[  2.01096744e+04   3.36256889e+04   9.00849739e+04 ...,   8.33687327e+04\n   2.12509031e+10   1.80231573e+04]\n6354887.416349604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.24816427e+04   3.84888794e+04   9.79955717e+04 ...,   8.10912588e+04\n   3.60845169e+10   2.11649990e+04]\n"
     ]
    }
   ],
   "source": [
    "# the format of using our validation function is:\n",
    "params = { \n",
    "                'eta'             : 0.4,\n",
    "                'nround'          : 3000,\n",
    "                'colsample_bytree': 0.9}\n",
    "\n",
    "\n",
    "validationErrorLog, trainingErrorLog = \\\n",
    "    crossValidationTimeSeries(dataTrainNumericalLog, labelTrainLog, 5, dataTestNumericalLog, params, True, True, 7, None)\n",
    "\n",
    "BSTLog, predictionLog = XGBoostTrain(dataTrainNumericalLog, dataTestNumericalLog, labelTrainLog, params, None, True, True, 7,\n",
    "                                     dataRawTestZero, 4, 1)\n",
    "print(predictionLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01805487  0.016893    0.01399839 ...,  0.01425388  0.00642419\n  0.02203563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.02045438  10.55812463  11.49267757 ...,  11.30333045  24.30912971\n   9.96010411]\n[  2.24816427e+04   3.84888794e+04   9.79955717e+04 ...,   8.10912588e+04\n   3.60845169e+10   2.11649990e+04]\n"
     ]
    }
   ],
   "source": [
    "BSTLog, predictionLog = XGBoostTrain(dataTrainNumericalLog, dataTestNumericalLog, labelTrainLog, params, None, True, True, 7,\n",
    "                                     dataRawTestZero, 4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble on many models from sklearn ML framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Floating number converted training and testing data for SKLearn Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = dataTrainNumerical.astype(float)\n",
    "testData = dataTestNumerical.astype(float)\n",
    "trainDataLog = dataTrainNumericalLog.astype(float)\n",
    "testDataLog = dataTestNumericalLog.astype(float)\n",
    "labelLog = labelTrainLog.astype(float)\n",
    "label = labelTrain.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation for sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalErrorSKLearn(prediction, realLabel, logBoolean):\n",
    "    # Root Mean Square Percentage Error (RMSPE), return a floating number\n",
    "    N = realLabel.shape[0]\n",
    "    error = 0\n",
    "    for i in range (N):\n",
    "        t = realLabel[i]\n",
    "        p = prediction[i]\n",
    "        if (logBoolean):\n",
    "            t = math.exp(t)\n",
    "            p = math.exp(p)\n",
    "        error += ((p-t)/t)**2\n",
    "    return float((error/N)**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidationSKLearnLinearModel(trainData, label, testData, kFold, logBoolean, SKLearn, **para):\n",
    "    # para is the optional parameters we want to pass in to respective SKLearn linear models\n",
    "    # SKLearn is the linear model we want to test out\n",
    "    N = trainData.shape[0]\n",
    "    resultant_validation_error_list = []\n",
    "    resultant_training_error_list = []\n",
    "    k = int(math.floor(N/(kFold+1)))\n",
    "    # print(k)\n",
    "    \n",
    "    for multiplier in range(kFold, 0, -1): # the training data we have is in the reverse of time\n",
    "        \n",
    "        trainingSet = trainData[k*multiplier:,:]\n",
    "        validationSet = trainData[k*(multiplier-1):(k*multiplier), :]\n",
    "        trainingLabel = label[k*multiplier:]\n",
    "        validationLabel = label[k*(multiplier-1):(k*multiplier)]\n",
    "        \n",
    "        # modelling and prediction\n",
    "        model = SKLearn(**para)\n",
    "        model.fit(X = trainingSet, y = trainingLabel)\n",
    "        validationPrediction = model.predict(X = validationSet)\n",
    "        trainingPrediction = model.predict(X = trainingSet)\n",
    "        prediction = model.predict(X = testData)\n",
    "        if (logBoolean):\n",
    "            prediction = restoreExponential(prediction)\n",
    "        \n",
    "        # error calculation\n",
    "        validationError = evalErrorSKLearn(validationPrediction,validationLabel, logBoolean)\n",
    "        trainingError = evalErrorSKLearn(trainingPrediction, trainingLabel, logBoolean)\n",
    "        \n",
    "        # result display\n",
    "        print(validationError)\n",
    "        print(trainingError)\n",
    "        print(prediction)\n",
    "        \n",
    "        resultant_validation_error_list.append(validationError)\n",
    "        resultant_training_error_list.append(trainingError)\n",
    "    # final validation error display\n",
    "    print(float(sum(resultant_validation_error_list)/len(resultant_validation_error_list))) #average validation error\n",
    "    return resultant_validation_error_list, resultant_training_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SKLearnTrain(trainData, label, testData, logBoolean, SKLearn, **para):\n",
    "    model = SKLearn(**para)\n",
    "    model.fit(X = trainData, y = label)\n",
    "    prediction = model.predict(X = testData)\n",
    "    if (logBoolean):\n",
    "        prediction = restoreExponential(prediction)\n",
    "    res = restoreZeroEntryInPrediction(prediction, dataRawTestZero, 4)\n",
    "    print (res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24837925804015928\n0.19991374477420376\n[  6186.99592668   7092.47147147   7618.9663437  ...,   8929.07481126\n  21922.6125       6186.99592668]\n0.31005533898112525\n0.22682466489437347\n[  6264.61376623   6264.61376623   7529.64416347 ...,   7529.64416347\n  21333.95850622   4372.13130194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2884385648702199\n0.23918900500799845\n[  6105.85766974   6335.20650458   7233.7828233  ...,   7233.7828233\n  21592.15274151   4196.76438506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27996529970447065\n0.23597325850415576\n[  5761.87511588   6574.18398701   6904.15039578 ...,   6904.15039578\n  21318.1236715    5463.76793184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27190291587779863\n0.24300075408346533\n[  5882.03434066   6114.08799146   6959.93963255 ...,   6959.93963255\n  20781.43099274   4087.43752799]\n0.27974827549475473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5675.04629695   5929.51126266   6796.09085862 ...,   6796.09085862\n  20338.8366718    5214.74197643]\n"
     ]
    }
   ],
   "source": [
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, label, testData, 5, False, ensemble.AdaBoostRegressor, \n",
    "                                      n_estimators=5, loss='exponential', learning_rate=0.3)\n",
    "prediction_ada = SKLearnTrain(trainData, label, testData, False, ensemble.AdaBoostRegressor,\n",
    "             n_estimators=5, loss='exponential', learning_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2369394094206078\n0.18591413517599126\n[  5828.12680566   6525.57289411   8407.89212128 ...,   7975.48130841\n  19360.99335747   5025.02611616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21228093446581162\n0.1953448537053547\n[  5744.27502495   6548.951484     7917.28567562 ...,   7870.14923118\n  18101.30396021   5078.1042291 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22028701809407575\n0.19675538423617775\n[  6007.07979412   6224.24278039   8148.31531699 ...,   7983.24601272\n  18445.43871875   5887.34889859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2398867180406944\n0.20139094825953133\n[  5590.56281256   6011.88955529   8001.1885401  ...,   7586.56018881\n  17723.69767976   4920.63338622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2115763999632089\n0.20171997398851083\n[  5814.67667927   5829.48557242   7872.04161958 ...,   7679.31835792\n  17308.11207879   5703.20577697]\n0.2241940959968797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5780.34624755   5978.11687436   7857.58098857 ...,   7799.56819865\n  17428.59383629   4757.55490402]\n"
     ]
    }
   ],
   "source": [
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainDataLog, labelLog, testDataLog, 5, True, ensemble.AdaBoostRegressor, \n",
    "                                      n_estimators=15, loss='exponential', learning_rate=0.5)\n",
    "prediction_ada = SKLearnTrain(trainDataLog, labelLog, testDataLog, True, ensemble.AdaBoostRegressor,\n",
    "             n_estimators=15, loss='exponential', learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09064786468009067\n0.02778703629313699\n[  4900.4   5959.7   8339.6 ...,   8465.8  29209.4   7198.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08956076235253671\n0.029506365986918043\n[  5221.1   6156.4   8715.3 ...,   8399.8  27933.4   6804.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09157308551071577\n0.03152245621880238\n[  5222.2   6254.8   8561.3 ...,   8440.3  27754.5   7193.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10828343717348433\n0.03194746791087417\n[  5012.3   6154.1   8482.  ...,   8338.5  27762.2   7291.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07910908474934915\n0.03414475611803851\n[  4937.225   5862.5     8573.8   ...,   8336.4    26914.3     7049.7  ]\n0.09183484689323532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4781.1   5933.1   8543.5 ...,   8354.2  26768.8   7236.5]\n"
     ]
    }
   ],
   "source": [
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, label, testData, 5, False, ensemble.BaggingRegressor,\n",
    "                                      n_estimators = 10, max_samples = 1.0, max_features = 1.0)\n",
    "prediction_bag = SKLearnTrain(trainData, label, testData, False, ensemble.BaggingRegressor,\n",
    "                              n_estimators = 10, max_samples = 1.0, max_features = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15807102148677118\n0.053024911538824104\n[  5166.63742551   6306.50684028   8790.51929804 ...,   7652.89466801\n  28213.1730355    6232.52535985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1628971013104506\n0.05139654257041346\n[  5347.09042554   6318.24221639   8567.8238541  ...,   7923.22428677\n  27358.41099625   6389.07738107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15449032697017795\n0.05092557910762369\n[  5624.75244256   5967.35865365   8966.62274873 ...,   7083.25979943\n  28409.04863852   6324.89200075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14882283184167744\n0.05020642372115892\n[  4555.76444769   5575.96545538   8584.75357549 ...,   8097.49876304\n  28081.75104706   6989.21528922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12012615194254325\n0.04936739338034706\n[  4487.75851454   5739.87073469   8269.38573548 ...,   8228.33783744\n  27506.91625712   7324.19386222]\n0.14888148671032408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4610.63143654   5830.50639573   8539.53623448 ...,   8245.06741716\n  27183.70289334   7564.2921274 ]\n"
     ]
    }
   ],
   "source": [
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainDataLog, labelLog, testDataLog, 5, True, ensemble.BaggingRegressor,\n",
    "                                      n_estimators = 15, max_samples = 1.0, max_features = 1.0)\n",
    "prediction_bag = SKLearnTrain(trainDataLog, labelLog, testDataLog, True, ensemble.BaggingRegressor,\n",
    "                              n_estimators = 15, max_samples = 1.0, max_features = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08841322208393496\n0.0052291569013782025\n[  5095.7   6127.1   8639.7 ...,   8419.3  29016.    7144.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09174280914382878\n0.006553311539475561\n[  5137.7   6285.9   8782.4 ...,   8631.9  26380.    7360.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09664937431541754\n0.007570218384525077\n[  5355.3   5958.5   8661.3 ...,   8217.3  26380.    7381.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11006925045243947\n0.00849858329337559\n[  4800.6   6176.8   7992.1 ...,   8301.3  27196.    7225.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08213457076616987\n0.010062349500653097\n[  5136.6   6067.2   8268.3 ...,   8038.7  26924.    7006.7]\n0.09380184535235812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4787.13333333   5851.8          8255.7        ...,   8120.8         26652.\n   7112.9       ]\n"
     ]
    }
   ],
   "source": [
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, label, testData, 5, False, ensemble.ExtraTreesRegressor)\n",
    "prediction_extra_tree = SKLearnTrain(trainData, label, testData, False, ensemble.ExtraTreesRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17158884196048207\n0.005208057355802583\n[  5534.34994762   6130.04615679   8865.31008874 ...,   7542.89351144\n  28364.41498728   6838.1463513 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17760327639172988\n0.00652632572332984\n[  5661.98540231   6155.03573953   9096.42989478 ...,   7675.94759824\n  27182.5528747    6854.21982611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16721555540921598\n0.0075331587029539866\n[  5584.63249735   6276.52988202   9367.50141637 ...,   7515.34685225\n  27382.31811728   6598.86543415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16352940772344873\n0.008445821240725588\n[  4633.17155046   5928.76821421   9397.33003762 ...,   6935.14666497\n  27141.52971795   6723.61675555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13794816942281987\n0.009986205960467995\n[  4729.68386558   5683.60922488  10215.19889843 ...,   8702.04878472\n  27114.92402154   7326.91645246]\n0.16357705018153929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4846.86301878   5815.24375336   9158.60131529 ...,   7848.82083006\n  26876.48697158   6986.36900361]\n"
     ]
    }
   ],
   "source": [
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainDataLog, labelLog, testDataLog, 5, True, ensemble.ExtraTreesRegressor)\n",
    "prediction_extra_tree = SKLearnTrain(trainDataLog, labelLog, testDataLog, True, ensemble.ExtraTreesRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08187041462833741\n0.06046621315913816\n[  5251.73740303   6344.71156277   8179.52105784 ...,   8507.18125063\n  27674.07247226   7235.24626303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08772149110951424\n0.06689914270362823\n[  5430.04000446   6313.47298446   8529.28830272 ...,   8578.73059588\n  27973.18919319   7326.21238095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1004724197594202\n0.07093225401450587\n[  5447.77275433   6526.7521546    8360.38123509 ...,   8565.03442424\n  27043.08745663   7424.37029525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1112324812256711\n0.07380173628714944\n[  5196.47636986   6096.33091825   8278.05271383 ...,   8470.02948646\n  26842.63890823   7318.19517458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08191468073602458\n0.07770111802065668\n[  5170.7547006    6021.35041141   8069.80273802 ...,   8238.38109336\n  26266.78276124   7061.12800618]\n0.09264229749179351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5080.92886499   5987.80080632   8271.56783726 ...,   8034.40722239\n  26431.55261939   7182.06966276]\n"
     ]
    }
   ],
   "source": [
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, label, testData, 5, False, ensemble.GradientBoostingRegressor)\n",
    "prediction_extra_tree = SKLearnTrain(trainData, label, testData, False, ensemble.GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16063190684486356\n0.1415225685926502\n[  5561.57790107   6251.12737776   7885.93553689 ...,   8174.07308375\n  25503.90673882   6271.68162482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1502795383013077\n0.14649535012908332\n[  5648.46217826   6220.53775828   8042.09070911 ...,   7969.52612183\n  25823.80157763   6402.53698555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16965587498384116\n0.14422467067004116\n[  5697.03523919   6373.44747594   7940.89920942 ...,   8247.55579154\n  25977.04960064   6219.16055078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17390270786640954\n0.14650494548024917\n[  5340.10590074   6157.05777289   7683.4722772  ...,   7913.07094693\n  25343.56316139   6170.68669627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1548587855850686\n0.14919420498419741\n[  5311.61592814   5981.28736903   7696.37304616 ...,   7716.85855836\n  25528.83797183   6060.24393533]\n0.1618657627162981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5261.768563     6110.62510035   7635.14480064 ...,   7815.58070686\n  24595.4405214    6052.71533632]\n"
     ]
    }
   ],
   "source": [
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainDataLog, labelLog, testDataLog, 5, True, ensemble.GradientBoostingRegressor)\n",
    "prediction_extra_tree = SKLearnTrain(trainDataLog, labelLog, testDataLog, True, ensemble.GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0898301877708355\n0.026416605965519188\n[  4846.26666667   6071.46666667   8349.         ...,   8446.          27721.\n   7344.33333333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08890272457319648\n0.028080125674715458\n[  5094.93333333   6175.13333333   8418.86666667 ...,   8610.66666667\n  28559.13333333   7082.86666667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09024594298512285\n0.029878155264736177\n[  5040.46666667   6029.53333333   8403.53333333 ...,   8324.13333333\n  28169.13333333   7094.2       ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10721420579049352\n0.03044896414053016\n[  5003.72777778   6160.46666667   8322.33333333 ...,   8170.33333333\n  26791.6          7267.13333333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07822186060522744\n0.032630229334477394\n[  4758.24444444   5958.46666667   8564.28888889 ...,   8167.06666667\n  26337.13333333   7116.4       ]\n0.09088298434497516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4655.85396825   5982.           8688.         ...,   8300.4         27550.4\n   7051.26666667]\n"
     ]
    }
   ],
   "source": [
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, label, testData, 5, False, ensemble.RandomForestRegressor,\n",
    "                                      n_estimators=15, max_features = 'auto')\n",
    "prediction_extra_tree = SKLearnTrain(trainData, label, testData, False, ensemble.RandomForestRegressor,\n",
    "                                     n_estimators=15, max_features = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16300365246500761\n0.05543382278264135\n[  5686.78092045   6179.57760319   8356.02918493 ...,   7593.6009153\n  27672.55946445   6367.01948284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1665575905636042\n0.05408727423804904\n[  5881.09395101   6428.48293929   9280.09572211 ...,   7630.30350646\n  28753.43499973   6464.25860669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15708347867300565\n0.053575219183924536\n[  5996.19946629   6370.56651947   8779.66000561 ...,   7742.26066771\n  27786.68191315   6712.7670496 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15083872889925143\n0.05300067315642066\n[  4963.06598675   5527.9017218    8494.83432573 ...,   7929.95291424\n  28522.39575289   7298.02725584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12211854389893957\n0.05228013824787389\n[  4572.17446612   5760.71100333   8513.52434325 ...,   8675.08172641\n  28616.51746958   7077.58858612]\n0.1519203988999617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4448.11205802   5987.45990771   8602.0066368  ...,   8314.75566008\n  28739.87203389   7140.22156749]\n"
     ]
    }
   ],
   "source": [
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainDataLog, labelLog, testDataLog, 5, True, ensemble.RandomForestRegressor)\n",
    "prediction_extra_tree = SKLearnTrain(trainDataLog, labelLog, testDataLog, True, ensemble.RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = os.path.join(directory_path, 'prediction_16.csv')\n",
    "writeToFile(prediction, filePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33450,)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
