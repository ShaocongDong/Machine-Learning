{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 (main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle team: < >\n",
    "#### Group: A0148008J\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing, Constants, and data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statement and extra libraries used\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statistics as st\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from datetime import datetime, date\n",
    "from datetime import timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant used in this homework\n",
    "DATE_TIME_FORMAT_DEV = \"%d/%m/%Y\"\n",
    "DATE_TIME_FORMAT_REAL = \"%Y-%m-%d\"\n",
    "DATE_TIME_FORMAT_WEEK = \"%Y-W%W-%w\"\n",
    "MIN_BOOLEAN_INDEX_TRAIN = 5\n",
    "MAX_BOOLEAN_INDEX_TRAIN = 8\n",
    "RAW_FEATURE_NUMBER_TRAIN = 9\n",
    "MIN_BOOLEAN_INDEX_TEST = 4\n",
    "MAX_BOOLEAN_INDEX_TEST = 7\n",
    "RAW_FEATURE_NUMBER_TEST = 8\n",
    "STORE_COMPETITION_SINCE_DEFAULT_TIME = datetime(2009, 3, 9, 18, 13, 5)\n",
    "STORE_NO_PROMOTION_SINCE_CONSTANT_TIME = datetime(2999, 1, 1, 0, 0, 0) # we assume this datetime is big enough\n",
    "STORE_NO_COMPETITION_SINCE_CONSTANT_TIME = datetime(2999, 1, 1, 0, 0, 0) # we assume this datetime is big enough\n",
    "STORE_NO_PROMO_INTERVAL_STRING = \"No Promotion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path processing\n",
    "current_pwd = os.getcwd()\n",
    "#directory_path = os.path.join(current_pwd, \"inpublic/homework3\")\n",
    "train_file_path = os.path.join(directory_path, \"train_v2.csv\")\n",
    "test_file_path = os.path.join(directory_path, \"test_v2.csv\")\n",
    "store_info_path = os.path.join(directory_path, \"store.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading and extraction function\n",
    "def dataLoadExtract(filePath, booleanMin, booleanMax, stateHolidayIndex):\n",
    "    rawDataMatrix = []\n",
    "    firstRow = True\n",
    "    \n",
    "    with open(filePath, newline='') as csvFile:\n",
    "        train_raw = csv.reader(csvFile, delimiter=',')\n",
    "        for row in train_raw:\n",
    "            if (firstRow):\n",
    "                rawDataMatrix.append(row)\n",
    "                firstRow = False\n",
    "            else:\n",
    "                currentRow = []\n",
    "                for i in range(len(row)):\n",
    "                    if (i == stateHolidayIndex):\n",
    "                        currentRow.append(row[i])\n",
    "                    elif booleanMin<=i<=booleanMax:\n",
    "                        if (row[i] == '0'):\n",
    "                            currentRow.append(False)\n",
    "                        else:\n",
    "                            currentRow.append(True)\n",
    "                    elif '-' in row[i]:\n",
    "                        currentRow.append(\n",
    "                            datetime.strptime(row[i], DATE_TIME_FORMAT_REAL).date())\n",
    "                    else:\n",
    "                        currentRow.append(int(row[i]))\n",
    "                rawDataMatrix.append(currentRow)\n",
    "    \n",
    "    headerRaw = rawDataMatrix[0] # a list containing all the headers as string\n",
    "    dataRaw = np.array(rawDataMatrix[1:]) # a numpy array with raw data\n",
    "    return headerRaw, dataRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeLoadExtract(filePath):\n",
    "    rawDataMatrix = []\n",
    "    firstRow = True\n",
    "    \n",
    "    with open(filePath, newline='') as csvFile:\n",
    "        train_raw = csv.reader(csvFile, delimiter=',')\n",
    "        for row in train_raw:\n",
    "            if (firstRow):\n",
    "                header = [\"Store Index\", \"Store Type\", \"Assortment\", \"Competition distance reciprocal\", \"Competition Since\",\n",
    "                          \"Promotion Since\", \"Promotion Interval\"]\n",
    "                rawDataMatrix.append(header)\n",
    "                firstRow = False\n",
    "            else:\n",
    "                currentRow = []\n",
    "                currentRow.append(int(row[0])) # store index\n",
    "                currentRow.append(row[1]) # store type\n",
    "                currentRow.append(row[2]) # assortment\n",
    "                \n",
    "                if (row[3] == \"\"): # competition distance reciprocal\n",
    "                    currentRow.append(0)\n",
    "                    currentRow.append(STORE_NO_COMPETITION_SINCE_CONSTANT_TIME)\n",
    "                else:\n",
    "                    currentRow.append(1.0/int(row[3])) \n",
    "                \n",
    "                    if (row[4] != \"\"): \n",
    "                        date_str = \"1/\"+row[4]+\"/\"+row[5]\n",
    "                        date_object = datetime.strptime(date_str, DATE_TIME_FORMAT_DEV)\n",
    "                        currentRow.append(date_object) # competition since time\n",
    "                    else:\n",
    "                        currentRow.append(STORE_COMPETITION_SINCE_DEFAULT_TIME)\n",
    "                \n",
    "                if (row[6] == \"0\"): # promotion specs\n",
    "                    currentRow.append(STORE_NO_PROMOTION_SINCE_CONSTANT_TIME)\n",
    "                    currentRow.append(STORE_NO_PROMO_INTERVAL_STRING)\n",
    "                else:\n",
    "                    date_str = row[8]+\"-W\"+row[7]+\"-0\"\n",
    "                    date_object = datetime.strptime(date_str, DATE_TIME_FORMAT_WEEK)\n",
    "                    currentRow.append(date_object)\n",
    "                    currentRow.append(row[9])\n",
    "\n",
    "                rawDataMatrix.append(currentRow)\n",
    "    \n",
    "    headerRaw = rawDataMatrix[0] # a list containing all the headers as string\n",
    "    dataRaw = np.array(rawDataMatrix[1:]) # a numpy array with raw data\n",
    "    return headerRaw, dataRaw\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading _(Test_v2.csv, train_v2.csv, store.csv)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/DongShaocong/Desktop/CS3244/MachineLearning-CS3244/inpublic/homework3/inpublic/homework3/train_v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0c594c787e98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Data loading and extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mheaderRawTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataRawTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataLoadExtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_BOOLEAN_INDEX_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_BOOLEAN_INDEX_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mheaderRawTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataRawTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataLoadExtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_BOOLEAN_INDEX_TEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_BOOLEAN_INDEX_TEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mheaderRawStore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataRawStore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstoreLoadExtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_info_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-655887cc2ea6>\u001b[0m in \u001b[0;36mdataLoadExtract\u001b[0;34m(filePath, booleanMin, booleanMax, stateHolidayIndex)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfirstRow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_raw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/DongShaocong/Desktop/CS3244/MachineLearning-CS3244/inpublic/homework3/inpublic/homework3/train_v2.csv'"
     ]
    }
   ],
   "source": [
    "# Data loading and extraction\n",
    "headerRawTrain, dataRawTrain = dataLoadExtract(train_file_path, MIN_BOOLEAN_INDEX_TRAIN, MAX_BOOLEAN_INDEX_TRAIN, 7)\n",
    "headerRawTest, dataRawTest = dataLoadExtract(test_file_path, MIN_BOOLEAN_INDEX_TEST, MAX_BOOLEAN_INDEX_TEST, 6)\n",
    "headerRawStore, dataRawStore = storeLoadExtract(store_info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation and features conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeInfoConverter (headerStore, dataStore, dataTrain):\n",
    "    newHeaderStore = headerStore.copy() # header processing\n",
    "    newHeaderStore += [\"Average Sales Without Promotion\",\n",
    "                       \"Average Sales With Promotion\",\n",
    "                       \"Average Sales\",\n",
    "                       \"Variance Sales Without Promotion\",\n",
    "                       \"Variance Sales With Promotion\",\n",
    "                       \"Variance Sales\",\n",
    "                       \"Average SC Ratio Without Promotion\",\n",
    "                       \"Average SC Ratio With Promotion\",\n",
    "                       \"Average SC Ratio\",\n",
    "                       \"Variance SC Ratio Without Promotion\",\n",
    "                       \"Variance SC Ratio With Promotion\",\n",
    "                       \"Variance SC Ratio\",\n",
    "                       \"Median Sales Without Promotion\",\n",
    "                       \"Median Sales With Promotion\",\n",
    "                       \"Median Sales\",\n",
    "                       \"Average Open Ratio\"]\n",
    "    \n",
    "    resultant_list = []\n",
    "    for row in dataStore:\n",
    "        currentRow = list(row.copy())\n",
    "        \n",
    "        # TO DO : maybe we should process the original raw categorical data here\n",
    "        \n",
    "        currentStore = int(row[0])\n",
    "        sales_list_without_promotion = []\n",
    "        sales_list_with_promotion = []\n",
    "        sc_ratio_list_without_promotion = []\n",
    "        sc_ratio_list_with_promotion = []\n",
    "        openDayCount = 0\n",
    "        entryCount = 0\n",
    "        \n",
    "        for salesRow in dataTrain:\n",
    "            if (int(salesRow[0]) == currentStore): # It's the store we want to analyze in this round\n",
    "                \n",
    "                # sales centric\n",
    "                if (salesRow[3] > 0):\n",
    "                    sc_ratio = salesRow[3]/salesRow[4]\n",
    "                    if (salesRow[6] is True):\n",
    "                        sales_list_with_promotion.append(salesRow[3])\n",
    "                        sc_ratio_list_with_promotion.append(sc_ratio)\n",
    "                    else:\n",
    "                        sales_list_without_promotion.append(salesRow[3])\n",
    "                        sc_ratio_list_without_promotion.append(sc_ratio)\n",
    "                \n",
    "                # open centric\n",
    "                if (salesRow[5] is True):\n",
    "                    openDayCount += 1\n",
    "            entryCount += 1\n",
    "        \n",
    "        # data processing and adding\n",
    "        currentRow.append(st.mean(sales_list_without_promotion))\n",
    "        currentRow.append(st.mean(sales_list_with_promotion))\n",
    "        currentRow.append(st.mean(sales_list_with_promotion+sales_list_without_promotion))\n",
    "        currentRow.append(st.variance(sales_list_without_promotion))\n",
    "        currentRow.append(st.variance(sales_list_with_promotion))\n",
    "        currentRow.append(st.variance(sales_list_with_promotion+sales_list_without_promotion))\n",
    "        currentRow.append(st.mean(sc_ratio_list_without_promotion))\n",
    "        currentRow.append(st.mean(sc_ratio_list_with_promotion))\n",
    "        currentRow.append(st.mean(sc_ratio_list_with_promotion+sc_ratio_list_without_promotion))\n",
    "        currentRow.append(st.variance(sc_ratio_list_without_promotion))\n",
    "        currentRow.append(st.variance(sc_ratio_list_with_promotion))\n",
    "        currentRow.append(st.variance(sc_ratio_list_with_promotion+sc_ratio_list_without_promotion))\n",
    "        currentRow.append(st.median(sales_list_without_promotion))\n",
    "        currentRow.append(st.median(sales_list_with_promotion))\n",
    "        currentRow.append(st.median(sales_list_with_promotion+sales_list_without_promotion))\n",
    "        currentRow.append(float(openDayCount/entryCount))\n",
    "        \n",
    "        # Recording of data\n",
    "        resultant_list.append(currentRow)\n",
    "    return newHeaderStore, np.array(resultant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header information processing\n",
    "headerStore, dataStore = storeInfoConverter(headerRawStore, dataRawStore, dataRawTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDataLabelSplit(dataTrain, headerTrain):\n",
    "    # This function makes the training data exactly in the same format of testing data read by our functions\n",
    "    dataTrainNew = np.hstack((dataTrain[:,:3], dataTrain[:,4:])) # remove the sales column\n",
    "    return dataTrainNew, headerTrain[:3]+headerTrain[4:], dataTrain[:,3] # return data, header and label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data label split\n",
    "dataTrain, headerTrain, labelTrain= trainDataLabelSplit(dataRawTrain, headerRawTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeInfoPromotionIntervalConverter(string):\n",
    "    if string == 'Jan,Apr,Jul,Oct':\n",
    "        return [1,4,7,10]\n",
    "    elif string == 'Feb,May,Aug,Nov':\n",
    "        return [2,5,8,11]\n",
    "    elif string == 'Mar,Jun,Sep,Dec':\n",
    "        return [3,6,9,12]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeNaturalJoin(dataStore, data, headerStore, header):\n",
    "    headerProcessed = [\"Day\",\n",
    "                       \"Month\",\n",
    "                       \"Month In Promotion\"]\n",
    "    newHeader=  header + headerProcessed + headerStore[1:] # no repetitive store index in header    \n",
    "    resultant_list = []\n",
    "    \n",
    "    for row in data:\n",
    "        currentIndex = row[0]\n",
    "        currentStoreInfo = dataStore[currentIndex-1,:] # get corresponding store entry with store index to be removed later\n",
    "        currentDate = row[2] # we will get the current datetime object\n",
    "        currentDay = currentDate.day # day value (integer)\n",
    "        currentMonth = currentDate.month # month value (integer)\n",
    "        monthInPromotion = currentMonth in storeInfoPromotionIntervalConverter(currentStoreInfo[6]) # boolean\n",
    "        currentRow = list(row.copy()) + [currentDay, currentMonth, monthInPromotion] # new entries \n",
    "        currentRow += list(currentStoreInfo[1:])\n",
    "        \n",
    "        resultant_list.append(currentRow)\n",
    "        \n",
    "    return newHeader, np.array(resultant_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data, and make it ready to use\n",
    "headerTrainClean, dataTrainClean = storeNaturalJoin(dataStore, dataTrain, headerStore, headerTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the testing features and make it ready to use\n",
    "headerTestClean, dataTestClean = storeNaturalJoin(dataStore, dataRawTest, headerStore, headerRawTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeZeroInDataTestClean(dataTestClean, openBooleanIndex):\n",
    "    resultantList = []\n",
    "    for row in dataTestClean:\n",
    "        if (row[openBooleanIndex] is True): # the store is opened on that day\n",
    "            resultantList.append(list(row))\n",
    "    return np.array(resultantList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeZeroInLabel(label):\n",
    "    result = []\n",
    "    for i in label:\n",
    "        if i != 0:\n",
    "            result.append(i)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find headerTestClean[4] is 'open'\n",
    "openBooleanIndex = 4\n",
    "dataTestRealClean = removeZeroInDataTestClean(dataTestClean, openBooleanIndex)\n",
    "dataTrainRealClean = removeZeroInDataTestClean(dataTrainClean, openBooleanIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove the zero entries in the labels\n",
    "labelTrainReal = removeZeroInLabel(labelTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost method - initial attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleFeatureOneHotKeyEncoder(feature_column_vector_train, feature_column_vector_test):\n",
    "    # numerical encoding\n",
    "    enc = LabelEncoder()\n",
    "    featureListTrain = list(feature_column_vector_train)\n",
    "    featureListTest = list(feature_column_vector_test)\n",
    "    \n",
    "    # we should fit the one has larger value set\n",
    "    if (len(set(featureListTrain)) > len(set(featureListTest))):\n",
    "        enc.fit(featureListTrain) \n",
    "    else:\n",
    "        enc.fit(featureListTest)\n",
    "    \n",
    "    labelEncodedFeatureTrain = enc.transform(featureListTrain).reshape(-1, 1)\n",
    "    labelEncodedFeatureTest = enc.transform(featureListTest).reshape(-1, 1)\n",
    "    \n",
    "    # oneHot encoding\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(labelEncodedFeatureTrain) # use train to fit the data\n",
    "    \n",
    "    # return order: train, test\n",
    "    return enc.transform(labelEncodedFeatureTrain).toarray(), enc.transform(labelEncodedFeatureTest).toarray()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleDateTimeColumnNumericalTransformer(dateTimeFeatureColumn):\n",
    "    processed = np.array([(t-datetime(1970,1,1)).total_seconds()/10**10 for t in dateTimeFeatureColumn])\n",
    "    return processed.reshape(len(processed),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleDateOnlyColumnNumericalTransformer(dateTimeFeatureColumn):\n",
    "    processed = np.array([(t-date(1970,1,1)).total_seconds()/10**9 for t in dateTimeFeatureColumn])\n",
    "    return processed.reshape(len(processed),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalTrainTransformation(headerTrain, dataTrain, headerTest, dataTest):\n",
    "    # This function prepares from the clean data to all numerical numpy array ready to be feed into DMatrix\n",
    "    # We will finally safely remove the store index column\n",
    "    \n",
    "    # TO DO : process and return the headerNew as well (for both train and test, they should be the same)\n",
    "    headerNew = [ \"DayOfWeek1\", \"DayOfWeek2\", \"DayOfWeek3\", \"DayOfWeek4\", \n",
    "                  \"DayOfWeek5\", \"DayOfWeek6\", \"DayOfWeek7\",\n",
    "                  \"Number of Customers\", \"Open -dummy -True by default\",\n",
    "                  \"PromoBoolean1\", \"PromoBoolean2\", \"StateHoliday1\",\n",
    "                  \"StateHoliday2\" ,\"StateHoliday3\",\n",
    "                  \"SchoolHolidayBoolean\", \"SchoolHolidayBoolean\",\n",
    "                  \"Day\", \"Month\", \"Month In Promotion 1\", \"Month In Promotion 2\",\n",
    "                  \"Store type 1\", \"store type 2\", \"store type 3\", \"store type 4\",\n",
    "                  \"assortmentType1\", \"assortmentType2\", \"assortmentType3\",\n",
    "                  \"Competition distance reciprocal\", \"competitionSinceDate\",\n",
    "                  \"Promotion Since Date\", \"promotionInterval1\", \"promotionInterval2\",\n",
    "                  \"promotionInterval3\", \"promotionInterval4\",\n",
    "                    'Average Sales Without Promotion',\n",
    "                 'Average Sales With Promotion',\n",
    "                 'Average Sales',\n",
    "                 'Variance Sales Without Promotion',\n",
    "                 'Variance Sales With Promotion',\n",
    "                 'Variance Sales',\n",
    "                 'Average SC Ratio Without Promotion',\n",
    "                 'Average SC Ratio With Promotion',\n",
    "                 'Average SC Ratio',\n",
    "                 'Variance SC Ratio Without Promotion',\n",
    "                 'Variance SC Ratio With Promotion',\n",
    "                 'Variance SC Ratio',\n",
    "                 'Median Sales Without Promotion',\n",
    "                 'Median Sales With Promotion',\n",
    "                 'Median Sales',\n",
    "                 'Average Open Ratio']\n",
    "    \n",
    "    # This function should get the training features dict for oneHotEncoded features (key -> number of types)\n",
    "    \n",
    "    \n",
    "    # Categorical features oneHotKey encoding column\n",
    "    \n",
    "    \n",
    "    dayOfWeekColumnTrain, dayOfWeekColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,1], dataTest[:,1])\n",
    "    \n",
    "    # the date shouldn't affect the prediction\n",
    "    dateDatetimeColumnTrain = singleDateOnlyColumnNumericalTransformer(dataTrain[:,2])\n",
    "    dateDatetimeColumnTest = singleDateOnlyColumnNumericalTransformer(dataTest[:,2])\n",
    "    \n",
    "    openBooleanTrain, openBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,4], dataTest[:,4]) \n",
    "    promoBooleanTrain, promoBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,5], dataTest[:,5]) \n",
    "    stateHolidayBooleanTrain, stateHolidayBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,6], dataTest[:,6])\n",
    "    schoolHolidayBooleanTrain, schoolHolidayBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,7], dataTest[:,7])\n",
    "    monthInPromotionBooleanTrain, monthInPromotionBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,10], dataTest[:,10])\n",
    "    storeTypeColumnTrain, storeTypeColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,11], dataTest[:,11])\n",
    "    assortmentTypeColumnTrain, assortmentTypeColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,12], dataTest[:,12])\n",
    "    \n",
    "    competitionSinceDatetimeColumnTrain = singleDateTimeColumnNumericalTransformer(dataTrain[:,14])\n",
    "    competitionSinceDatetimeColumnTest = singleDateTimeColumnNumericalTransformer(dataTest[:,14])\n",
    "    promotionSinceDatetimeColumnTrain = singleDateTimeColumnNumericalTransformer(dataTrain[:,15])\n",
    "    promotionSinceDatetimeColumnTest = singleDateTimeColumnNumericalTransformer(dataTest[:,15])\n",
    "    \n",
    "    promotionIntervalColumnTrain, promotionIntervalColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,16], dataTest[:,16])\n",
    "    \n",
    "    # use numpy.column_stack to accomplish column and matrix side by side stacking\n",
    "    resultantArrayTrain = np.column_stack((dayOfWeekColumnTrain,                                      \n",
    "                                      dataTrain[:,3],\n",
    "                                      openBooleanTrain,\n",
    "                                      promoBooleanTrain,\n",
    "                                      stateHolidayBooleanTrain,\n",
    "                                      schoolHolidayBooleanTrain,\n",
    "                                      dataTrain[:,8:10],\n",
    "                                      monthInPromotionBooleanTrain,\n",
    "                                      storeTypeColumnTrain,\n",
    "                                      assortmentTypeColumnTrain,\n",
    "                                      dataTrain[:,13],\n",
    "                                      competitionSinceDatetimeColumnTrain,\n",
    "                                      promotionSinceDatetimeColumnTrain,\n",
    "                                      promotionIntervalColumnTrain,\n",
    "                                      dataTrain[:,17:]))\n",
    "    \n",
    "    resultantArrayTest = np.column_stack((dayOfWeekColumnTest,\n",
    "                                      dataTest[:,3],\n",
    "                                      openBooleanTest,\n",
    "                                      promoBooleanTest,\n",
    "                                      stateHolidayBooleanTest,\n",
    "                                      schoolHolidayBooleanTest,\n",
    "                                      dataTest[:,8:10],\n",
    "                                      monthInPromotionBooleanTest,\n",
    "                                      storeTypeColumnTest,\n",
    "                                      assortmentTypeColumnTest,\n",
    "                                      dataTest[:,13],\n",
    "                                      competitionSinceDatetimeColumnTest,\n",
    "                                      promotionSinceDatetimeColumnTest,\n",
    "                                      promotionIntervalColumnTest,\n",
    "                                      dataTest[:,17:]))\n",
    "    \n",
    "    return resultantArrayTrain, resultantArrayTest, headerNew # order: train, test, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataTrainClean categorical feature pre processing\n",
    "dataTrainNumerical, dataTestNumerical, headerWhole = \\\n",
    "    numericalTrainTransformation(headerTrainClean, dataTrainRealClean, \n",
    "                                 headerTestClean, dataTestRealClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and label into xgb.DMatrix\n",
    "dTrain = xgb.DMatrix(dataTrainNumerical, label=labelTrainReal)\n",
    "dTest = xgb.DMatrix(dataTestNumerical)\n",
    "dTrainWithoutLabel = xgb.DMatrix(dataTrainNumerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restoreZeroEntryInPrediction(prediction, dataTestCleanWithZero, openBooleanIndex):\n",
    "    resultantList= []\n",
    "    predictionIndex = 0\n",
    "    for i in range(dataTestCleanWithZero.shape[0]):\n",
    "        if (dataTestCleanWithZero[i][openBooleanIndex] is True): # the store is open, put our prediction inside\n",
    "            resultantList.append(prediction[predictionIndex])\n",
    "            predictionIndex += 1 # update prediction index to the next prediction point\n",
    "        else:\n",
    "            resultantList.append(0) # the store is closed, append zero and do not update the index\n",
    "    return np.array(resultantList).reshape(len(resultantList),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFile(numpyArray, filePath):\n",
    "    with open(filePath, 'w') as csvFile:\n",
    "        prediction_writer = csv.writer(csvFile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        prediction_writer.writerow([\"\\\"Id\\\"\", \"\\\"Sales\\\"\"])\n",
    "        for i in range(numpyArray.shape[0]):\n",
    "            prediction_writer.writerow([i+1, numpyArray[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how we write to file:\n",
    "* filePath = os.path.join(directory_path, \"prediction_3.csv\")\n",
    "* writeToFile(prediction, filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions:\n",
    "* In this attempt, we try with customized objective functions and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined objective function -> to feed in xgboost, return numpy array\n",
    "def objectiveFunction(prediction, dTrain):\n",
    "    labels = dTrain.get_label()\n",
    "    scale = labels.max()\n",
    "    grad = -1/labels + prediction/(labels**2)\n",
    "    hess = 1/(labels**2)\n",
    "    return scale*grad, scale*hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root Mean Square Percentage Error (RMSPE) return a floating number\n",
    "def evalError(prediction, dTrain):    \n",
    "    labels = dTrain.get_label()\n",
    "    N = labels.shape[0]\n",
    "    error = 0\n",
    "    for i in range (N):\n",
    "        t = labels[i]\n",
    "        p = prediction[i]\n",
    "        error += ((p-t)/t)**2\n",
    "    return 'error', float((error/N)**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RmspeObjective:\n",
    "\n",
    "    hessian = None\n",
    "\n",
    "    def __call__(self, predicted, target):\n",
    "        target = target.get_label()\n",
    "        # I suspect this is necessary since XGBoost is using 32 bit floats\n",
    "        # and I'm getting some sort of under/overflow, but that's just a guess\n",
    "        if self.hessian is None:\n",
    "            scale = target.max()\n",
    "            valid = (target > 0)\n",
    "            self.hessian = np.where(valid, 1.0 / (target / scale)**2, 0)  \n",
    "        grad = (predicted - target) * self.hessian\n",
    "        # I suspect (from experiment not from actually reading the relevant paper)\n",
    "        # that what is important is the ratio of grad to hess.  That's why (I think)\n",
    "        # I can get away with returning these values, which should be divided by\n",
    "        # scale**2\n",
    "        return grad, self.hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorScore(dTrainWithoutLabel, booster, label):\n",
    "    prediction = booster.predict(dTrainWithoutLabel)\n",
    "    N = label.shape[0]\n",
    "    error = 0\n",
    "    for i in range (N):\n",
    "        t = label[i]\n",
    "        p = prediction[i]\n",
    "        error += ((p-t)/t)**2\n",
    "    return float((error/N)**(1/2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation - one round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {      'booster'         :'gbtree',\n",
    "                'objective'       :'reg:linear',\n",
    "                'nround'          : 3000,\n",
    "                'colsample_bytree': 0.4,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainXGBoost(trainingSet, label, validationSet, validationLabel, testingData, para):\n",
    "    dTrain = xgb.DMatrix(trainingSet, label=label)\n",
    "    dTest = xgb.DMatrix(testingData)\n",
    "    dValidation = xgb.DMatrix(validationSet)\n",
    "    dTrainWithoutLabel = xgb.DMatrix(trainingSet)\n",
    "    bst = xgb.train(params=para, dtrain=dTrain, obj = RmspeObjective())\n",
    "    prediction = bst.predict(dTest)\n",
    "    error = errorScore(dValidation, bst, validationLabel)\n",
    "    trainError = errorScore(dTrainWithoutLabel, bst, label)\n",
    "    print(\"validation error\" + str(error))\n",
    "    print(\"training error\" + str(trainError))\n",
    "    print(prediction)\n",
    "    return prediction, bst, error, trainError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error0.10588355152661916\n",
      "training error0.0943691597049144\n",
      "[  4975.296875     5189.609375     7529.80810547 ...,   8129.765625\n",
      "  20750.515625     6726.33300781]\n"
     ]
    }
   ],
   "source": [
    "# use the first 6000 rows of the training data as validation set\n",
    "trainingSet = dataTrainNumerical[9114:,:]\n",
    "validationSet = dataTrainNumerical[:9114,:]\n",
    "label = labelTrainReal[9114:]\n",
    "validationLabel = labelTrainReal[:9114]\n",
    "prediction , bst, validationError, trainingError= \\\n",
    "    trainXGBoost(trainingSet, label, validationSet, \n",
    "                 validationLabel, dataTestNumerical,params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation -> method for time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidationTimeSeries(dataTrainNumerical, label, kFold, dataTestNumerical, para):\n",
    "    N = dataTrainNumerical.shape[0]\n",
    "    resultant_validation_error_list = []\n",
    "    resultant_training_error_list = []\n",
    "    k = int(math.floor(N/(kFold+1)))\n",
    "    # print(k)\n",
    "    \n",
    "    for multiplier in range(kFold, 0, -1): # the training data we have is in the reverse of time\n",
    "        \n",
    "        trainingSet = dataTrainNumerical[k*multiplier:,:]\n",
    "        validationSet = dataTrainNumerical[k*(multiplier-1):(k*multiplier), :]\n",
    "        trainingLabel = label[k*multiplier:]\n",
    "        validationLabel = label[k*(multiplier-1):(k*multiplier)]\n",
    "        \n",
    "        prediction , bst, validationError, trainingError= \\\n",
    "            trainXGBoost(trainingSet, trainingLabel, validationSet, \n",
    "                         validationLabel, dataTestNumerical, para)\n",
    "        \n",
    "        resultant_validation_error_list.append(validationError)\n",
    "        resultant_training_error_list.append(trainingError)\n",
    "    \n",
    "    print(float(sum(resultant_validation_error_list)/len(resultant_validation_error_list))) #average validation error\n",
    "    return resultant_validation_error_list, resultant_training_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error0.19940895403592448\n",
      "training error0.07099890731883694\n",
      "[  6279.11669922   6625.29199219   9298.49121094 ...,   8210.61621094\n",
      "  25093.01757812   7573.85888672]\n",
      "validation error0.22758069271060466\n",
      "training error0.07677104624116776\n",
      "[  5705.67285156   5945.33251953   8395.49414062 ...,   7943.45556641\n",
      "  23959.03515625   7265.11572266]\n",
      "validation error0.15874419934571501\n",
      "training error0.08910849661222453\n",
      "[  5066.10205078   5930.06396484   7551.01318359 ...,   8512.94824219\n",
      "  24706.85351562   7238.41308594]\n",
      "validation error0.17785212265774408\n",
      "training error0.094826196747337\n",
      "[  4825.12548828   5413.35595703   7734.078125   ...,   7989.33056641\n",
      "  23360.56835938   7162.94873047]\n",
      "validation error0.10588355152661916\n",
      "training error0.0943691597049144\n",
      "[  4975.296875     5189.609375     7529.80810547 ...,   8129.765625\n",
      "  20750.515625     6726.33300781]\n",
      "0.17389390405532149\n"
     ]
    }
   ],
   "source": [
    "# the format of using our validation function is:\n",
    "validationError, trainingError = \\\n",
    "    crossValidationTimeSeries(dataTrainNumerical, labelTrainReal, 5, dataTestNumerical, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
