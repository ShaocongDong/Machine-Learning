{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework 3 (main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle team: HEHEDA\n",
    "#### Group: A0148008J, A0141132B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing, Constants, and data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import statement and extra libraries used\n",
    "%config IPCompleter.greedy=True\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statistics as st\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from datetime import datetime, date\n",
    "from matplotlib import cm\n",
    "from datetime import timedelta\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constant used in this homework\n",
    "DATE_TIME_FORMAT_DEV = \"%d/%m/%Y\"\n",
    "DATE_TIME_FORMAT_REAL = \"%Y-%m-%d\"\n",
    "DATE_TIME_FORMAT_WEEK = \"%Y-W%W-%w\"\n",
    "MIN_BOOLEAN_INDEX_TRAIN = 5\n",
    "MAX_BOOLEAN_INDEX_TRAIN = 8\n",
    "RAW_FEATURE_NUMBER_TRAIN = 9\n",
    "MIN_BOOLEAN_INDEX_TEST = 4\n",
    "MAX_BOOLEAN_INDEX_TEST = 7\n",
    "RAW_FEATURE_NUMBER_TEST = 8\n",
    "STORE_COMPETITION_SINCE_DEFAULT_TIME = date(2009, 3, 9)\n",
    "STORE_NO_PROMOTION_SINCE_CONSTANT_TIME = date(2999, 1, 1) # we assume this datetime is big enough\n",
    "STORE_NO_COMPETITION_SINCE_CONSTANT_TIME = date(2999, 1, 1) # we assume this datetime is big enough\n",
    "STORE_NO_PROMO_INTERVAL_STRING = \"No Promotion\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File path conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File path processing\n",
    "directory_path = current_pwd = os.getcwd()\n",
    "directory_path = os.path.join(directory_path, \"inpublic/homework3\")\n",
    "train_file_path = os.path.join(directory_path, \"train_v2.csv\")\n",
    "test_file_path = os.path.join(directory_path, \"test_v2.csv\")\n",
    "store_info_path = os.path.join(directory_path, \"store.csv\")\n",
    "cheat_path = os.path.join(directory_path, \"Cheat.csv\")\n",
    "cheat_total_path = os.path.join(directory_path, \"CheatTotal.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write bcak to csv function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeToFile(numpyArray, filePath):\n",
    "    with open(filePath, 'w') as csvFile:\n",
    "        prediction_writer = csv.writer(csvFile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        prediction_writer.writerow([\"\\\"Id\\\"\", \"\\\"Sales\\\"\"])\n",
    "        for i in range(numpyArray.shape[0]):\n",
    "            prediction_writer.writerow([i+1, numpyArray[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data loading and extraction function, include sales = zeros for restoring entries later\n",
    "def dataLoadExtract(filePath, logBoolean):\n",
    "    rawDataMatrix = []\n",
    "    firstRow = True\n",
    "    header = []\n",
    "    \n",
    "    with open(filePath, newline='') as csvFile:\n",
    "        train_raw = csv.reader(csvFile, delimiter=',')\n",
    "        for row in train_raw:\n",
    "            if (firstRow):\n",
    "                firstRow = False\n",
    "                header = row                \n",
    "            else:\n",
    "                currentRow = []\n",
    "                for i in range(len(row)):\n",
    "                    \n",
    "                    ## if doing log transform on sales and customers\n",
    "                    if (header[i] in ['Customers', 'Sales']) and (logBoolean is True):\n",
    "                        if (int(row[i]) == 0):\n",
    "                            currentRow.append(int(row[i]))\n",
    "                        else:\n",
    "                            currentRow.append(math.log(int(row[i]))) \n",
    "\n",
    "                    # Convert 0, 1 to boolean\n",
    "                    elif (header[i] in ['Open', 'Promo', 'SchoolHoliday']):\n",
    "                        if (row[i] == '0'):\n",
    "                            currentRow.append(False)\n",
    "                        else:\n",
    "                            currentRow.append(True)\n",
    "                    # Convert Date to Date object in python\n",
    "                    elif (header[i] == 'Date'):\n",
    "                        currentRow.append(\n",
    "                            datetime.strptime(row[i], DATE_TIME_FORMAT_REAL).date())\n",
    "                    \n",
    "                    elif (header[i] == 'StateHoliday'):\n",
    "                        currentRow.append(row[i])  \n",
    "                        \n",
    "                    else:\n",
    "                        currentRow.append(int(row[i]))\n",
    "                rawDataMatrix.append(currentRow)\n",
    "    \n",
    "    dataRaw = np.array(rawDataMatrix[0:]) # a numpy array with raw data\n",
    "    return header, dataRaw\n",
    "\n",
    "def storeLoadExtract(filePath):\n",
    "    rawDataMatrix = []\n",
    "    firstRow = True\n",
    "    \n",
    "    with open(filePath, newline='') as csvFile:\n",
    "        train_raw = csv.reader(csvFile, delimiter=',')\n",
    "        for row in train_raw:\n",
    "            if (firstRow):\n",
    "                header = [\"Store Index\", \"Store Type\", \"Assortment\", \"Competition distance reciprocal\", \"Competition Since\",\n",
    "                          \"Promotion Since\", \"Promotion Interval\"]\n",
    "                rawDataMatrix.append(header)\n",
    "                firstRow = False\n",
    "            else:\n",
    "                currentRow = []\n",
    "                currentRow.append(int(row[0])) # store index\n",
    "                currentRow.append(row[1]) # store type\n",
    "                currentRow.append(row[2]) # assortment\n",
    "                \n",
    "                if (row[3] == \"\"): # competition distance reciprocal\n",
    "                    currentRow.append(0)\n",
    "                    currentRow.append(STORE_NO_COMPETITION_SINCE_CONSTANT_TIME)\n",
    "                else:\n",
    "                    currentRow.append(1.0/int(row[3])) \n",
    "                \n",
    "                    if (row[4] != \"\"): \n",
    "                        date_str = \"1/\"+row[4]+\"/\"+row[5]\n",
    "                        date_object = datetime.strptime(date_str, DATE_TIME_FORMAT_DEV).date()\n",
    "                        currentRow.append(date_object) # competition since time\n",
    "                    else:\n",
    "                        currentRow.append(STORE_COMPETITION_SINCE_DEFAULT_TIME)\n",
    "                \n",
    "                if (row[6] == \"0\"): # promotion specs\n",
    "                    currentRow.append(STORE_NO_PROMOTION_SINCE_CONSTANT_TIME)\n",
    "                    currentRow.append(STORE_NO_PROMO_INTERVAL_STRING)\n",
    "                else:\n",
    "                    date_str = row[8]+\"-W\"+row[7]+\"-0\"\n",
    "                    date_object = datetime.strptime(date_str, DATE_TIME_FORMAT_WEEK).date()\n",
    "                    currentRow.append(date_object)\n",
    "                    currentRow.append(row[9])\n",
    "\n",
    "                rawDataMatrix.append(currentRow)\n",
    "    \n",
    "    headerRaw = rawDataMatrix[0] # a list containing all the headers as string\n",
    "    dataRaw = np.array(rawDataMatrix[1:]) # a numpy array with raw data\n",
    "    return headerRaw, dataRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and extraction\n",
    "headerRawTrainZero, dataRawTrainZero = dataLoadExtract(train_file_path, False)\n",
    "headerRawTestZero, dataRawTestZero = dataLoadExtract(test_file_path, False)\n",
    "headerRawTrainZeroLog, dataRawTrainZeroLog = dataLoadExtract(train_file_path, True)\n",
    "headerRawTestZeroLog, dataRawTestZeroLog = dataLoadExtract(test_file_path, True)\n",
    "headerRawStore, dataRawStore = storeLoadExtract(store_info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading and Zero (not open) Record Removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeZeros(header, data):\n",
    "    rawList = data.tolist()\n",
    "    resultant_list = []\n",
    "    i = header.index('Open')\n",
    "    for row in rawList:\n",
    "        if (row[i] is True):\n",
    "            resultant_list.append(row)\n",
    "    return header, np.array(resultant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove zero (not open sales record) from the data raw read from the file\n",
    "headerRawTrain, dataRawTrain = removeZeros(headerRawTrainZero, dataRawTrainZero)\n",
    "headerRawTest, dataRawTest = removeZeros(headerRawTestZero, dataRawTestZero)\n",
    "headerRawTrainLog, dataRawTrainLog = removeZeros(headerRawTrainZeroLog, dataRawTrainZeroLog)\n",
    "headerRawTestLog, dataRawTestLog = removeZeros(headerRawTestZeroLog, dataRawTestZeroLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation and features conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the steps in this section:\n",
    "1. We gather store information for each store in the store list using the training data\n",
    "2. We split the training data into features and labels\n",
    "3. We combine the store information and the training features to get the full information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storeInfoConverter (headerStore, dataStore, dataTrain):\n",
    "    newHeaderStore = headerStore.copy() # header processing\n",
    "    newHeaderStore += [\"Average Sales Without Promotion\",\n",
    "                       \"Average Sales With Promotion\",\n",
    "                       \"Average Sales\",\n",
    "                       \"Variance Sales Without Promotion\",\n",
    "                       \"Variance Sales With Promotion\",\n",
    "                       \"Variance Sales\",\n",
    "                       \"Average SC Ratio Without Promotion\",\n",
    "                       \"Average SC Ratio With Promotion\",\n",
    "                       \"Average SC Ratio\",\n",
    "                       \"Variance SC Ratio Without Promotion\",\n",
    "                       \"Variance SC Ratio With Promotion\",\n",
    "                       \"Variance SC Ratio\",\n",
    "                       \"Median Sales Without Promotion\",\n",
    "                       \"Median Sales With Promotion\",\n",
    "                       \"Median Sales\",\n",
    "                       \"Average Open Ratio\"]\n",
    "    \n",
    "    resultant_list = []\n",
    "    for row in dataStore:\n",
    "        currentRow = list(row.copy())\n",
    "        \n",
    "        # TO DO : maybe we should process the original raw categorical data here\n",
    "        \n",
    "        currentStore = int(row[0])\n",
    "        sales_list_without_promotion = []\n",
    "        sales_list_with_promotion = []\n",
    "        sc_ratio_list_without_promotion = []\n",
    "        sc_ratio_list_with_promotion = []\n",
    "        openDayCount = 0\n",
    "        entryCount = 0\n",
    "        \n",
    "        for salesRow in dataTrain:\n",
    "            if (int(salesRow[0]) == currentStore): # It's the store we want to analyze in this round\n",
    "                \n",
    "                # sales centric\n",
    "                if (salesRow[3] > 0):\n",
    "                    sc_ratio = salesRow[3]/salesRow[4]\n",
    "                    if (salesRow[6] is True):\n",
    "                        sales_list_with_promotion.append(salesRow[3])\n",
    "                        sc_ratio_list_with_promotion.append(sc_ratio)\n",
    "                    else:\n",
    "                        sales_list_without_promotion.append(salesRow[3])\n",
    "                        sc_ratio_list_without_promotion.append(sc_ratio)\n",
    "                \n",
    "                # open centric\n",
    "                if (salesRow[5] is True):\n",
    "                    openDayCount += 1\n",
    "            entryCount += 1\n",
    "        \n",
    "        # data processing and adding\n",
    "        currentRow.append(st.mean(sales_list_without_promotion))\n",
    "        currentRow.append(st.mean(sales_list_with_promotion))\n",
    "        currentRow.append(st.mean(sales_list_with_promotion+sales_list_without_promotion))\n",
    "        currentRow.append(st.variance(sales_list_without_promotion))\n",
    "        currentRow.append(st.variance(sales_list_with_promotion))\n",
    "        currentRow.append(st.variance(sales_list_with_promotion+sales_list_without_promotion))\n",
    "        currentRow.append(st.mean(sc_ratio_list_without_promotion))\n",
    "        currentRow.append(st.mean(sc_ratio_list_with_promotion))\n",
    "        currentRow.append(st.mean(sc_ratio_list_with_promotion+sc_ratio_list_without_promotion))\n",
    "        currentRow.append(st.variance(sc_ratio_list_without_promotion))\n",
    "        currentRow.append(st.variance(sc_ratio_list_with_promotion))\n",
    "        currentRow.append(st.variance(sc_ratio_list_with_promotion+sc_ratio_list_without_promotion))\n",
    "        currentRow.append(st.median(sales_list_without_promotion))\n",
    "        currentRow.append(st.median(sales_list_with_promotion))\n",
    "        currentRow.append(st.median(sales_list_with_promotion+sales_list_without_promotion))\n",
    "        currentRow.append(float(openDayCount/entryCount))\n",
    "        \n",
    "        # Recording of data\n",
    "        resultant_list.append(currentRow)\n",
    "    return newHeaderStore, np.array(resultant_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store information getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header information processing\n",
    "headerStore, dataStore = storeInfoConverter(headerRawStore, dataRawStore, dataRawTrain)\n",
    "# logged header information processing\n",
    "headerStoreLog, dataStoreLog = storeInfoConverter(headerRawStore, dataRawStore, dataRawTrainLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the label and training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainDataLabelSplit(dataTrain, headerTrain):\n",
    "    # This function makes the training data exactly in the same format of testing data read by our functions\n",
    "    dataTrainNew = np.hstack((dataTrain[:,:3], dataTrain[:,4:])) # remove the sales column\n",
    "    return dataTrainNew, headerTrain[:3]+headerTrain[4:], dataTrain[:,3] # return data, header and label\n",
    "\n",
    "# train data label split\n",
    "dataTrain, headerTrain, labelTrain = trainDataLabelSplit(dataRawTrain, headerRawTrain)\n",
    "dataTrainLog, headerTrainLog, labelTrainLog = trainDataLabelSplit(dataRawTrainLog, headerRawTrainLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store info joining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storeInfoPromotionIntervalConverter(string):\n",
    "    # this method is used to help us determine if the current month is inside the promotion month\n",
    "    if string == 'Jan,Apr,Jul,Oct':\n",
    "        return [1,4,7,10]\n",
    "    elif string == 'Feb,May,Aug,Nov':\n",
    "        return [2,5,8,11]\n",
    "    elif string == 'Mar,Jun,Sep,Dec':\n",
    "        return [3,6,9,12]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storeNaturalJoin(dataStore, data, headerStore, header):\n",
    "    # In this function, we modify it so that it will join up the promotion `True` store with the info with promotion\n",
    "    # And the non promotion store with the info with non promotion (info being the average, variance and etc)\n",
    "    \n",
    "    # Header processing\n",
    "    headerProcessed = [\"Day\",\n",
    "                       \"Month\",\n",
    "                       \"Year\",\n",
    "                       \"Month In Promotion\"]\n",
    "    newHeader =  [header[1]] + header[3:] + headerProcessed + headerStore[1:4] +\\\n",
    "                 [\"Competition Since Day Count\", \"Promotion Since Day Count\", headerStore[6]] # no repetitive store index in header    \n",
    "    newHeader += [headerStore[9], headerStore[12], headerStore[15], \n",
    "                      headerStore[18], headerStore[21], headerStore[22]]\n",
    "    \n",
    "    \n",
    "    resultant_list = []\n",
    "    \n",
    "    for row in data:\n",
    "        currentIndex = row[0]\n",
    "        currentStoreInfo = dataStore[currentIndex-1,:] # get corresponding store entry with store index to be removed later\n",
    "        currentDate = row[2] # we will get the current datetime object\n",
    "        currentDay = currentDate.day # day value (integer)\n",
    "        currentMonth = currentDate.month # month value (integer)\n",
    "        currentYear = currentDate.year # year value (integer)\n",
    "        monthInPromotion = currentMonth in storeInfoPromotionIntervalConverter(currentStoreInfo[6]) # boolean\n",
    "        \n",
    "        listRow = row.copy().tolist()\n",
    "        listRow = [listRow[1],] + listRow[3:]\n",
    "        currentRow = listRow + [currentDay, currentMonth, currentYear, monthInPromotion] # new entries \n",
    "        \n",
    "        competitionSinceDate = currentStoreInfo[4] # competition since date\n",
    "        promotionSinceDate = currentStoreInfo[5] # promotion since date\n",
    "        competitionPastDayCount = (currentDate - competitionSinceDate).days\n",
    "        competitionPastDayCount = 0 if competitionPastDayCount < 0 else competitionPastDayCount\n",
    "        promotionPastDayCount = (currentDate - promotionSinceDate).days\n",
    "        promotionPastDayCount = 0 if promotionPastDayCount < 0 else promotionPastDayCount\n",
    "        \n",
    "        #currentRow += list(currentStoreInfo[1:])\n",
    "        # concatenate the relevant info (distinguishing promotion and non-promotion)\n",
    "        promotionBoolean = row[5]\n",
    "        constantInfo = [currentStoreInfo[1], currentStoreInfo[2], currentStoreInfo[3],\n",
    "                            competitionPastDayCount, promotionPastDayCount, currentStoreInfo[6]]\n",
    "        noPromotionList = [currentStoreInfo[7], currentStoreInfo[10], currentStoreInfo[13],\n",
    "                           currentStoreInfo[16], currentStoreInfo[19], currentStoreInfo[22]]\n",
    "        promotionList = [currentStoreInfo[8], currentStoreInfo[11], currentStoreInfo[14],\n",
    "                         currentStoreInfo[17], currentStoreInfo[20], currentStoreInfo[22]]\n",
    "        #totalList = [currentStoreInfo[9], currentStoreInfo[12], currentStoreInfo[15],\n",
    "        #             currentStoreInfo[18], currentStoreInfo[21], currentStoreInfo[22]]\n",
    "        \n",
    "        # Check and append differently\n",
    "        if promotionBoolean:\n",
    "            currentRow += constantInfo + promotionList\n",
    "        else:\n",
    "            currentRow += constantInfo + noPromotionList\n",
    "        \n",
    "        #currentRow += constantInfo + totalList\n",
    "        \n",
    "        # Problem here: we have to append one object to make the numpy array conversion correct\n",
    "        currentRow.append(object()) # random python object\n",
    "        \n",
    "        resultant_list.append(currentRow)\n",
    "        \n",
    "    # manually remove this object to keep the correctness of the data\n",
    "    resultNumpyArray = np.array(resultant_list)\n",
    "    resultNumpyArray = resultNumpyArray[:,0: resultNumpyArray.shape[1]-1]\n",
    "    \n",
    "    return newHeader, resultNumpyArray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine with store information to get full information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the logged data and make it ready to use\n",
    "headerTrainCleanLog, dataTrainCleanLog = storeNaturalJoin(dataStoreLog, dataTrainLog, headerStoreLog, headerTrainLog)\n",
    "headerTrainClean, dataTrainClean = storeNaturalJoin(dataStore, dataTrain, headerStore, headerTrain)\n",
    "headerTestClean, dataTestClean = storeNaturalJoin(dataStore, dataRawTest, headerStore, headerRawTest)\n",
    "headerTestCleanLog, dataTestCleanLog = storeNaturalJoin(dataStoreLog, dataRawTestLog, headerStoreLog, headerRawTestLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot key encoding of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singleFeatureOneHotKeyEncoder(feature_column_vector_train, feature_column_vector_test, *dayOfWeek):\n",
    "    # Problem here is that, the testing and training data may not coincide\n",
    "    # We should write separate functions to deal with data (day, month, year)\n",
    "    \n",
    "    # numerical encoding\n",
    "    enc = LabelEncoder()\n",
    "    featureListTrain = (feature_column_vector_train).tolist()\n",
    "    featureListTest = (feature_column_vector_test).tolist()\n",
    "    \n",
    "    if (len(dayOfWeek) > 0):\n",
    "        dayOfWeek = dayOfWeek[0]\n",
    "        featureListTrain = [dayOfWeek if x == dayOfWeek else 0 for x in featureListTrain]\n",
    "        featureListTest = [dayOfWeek if x == dayOfWeek else 0 for x in featureListTest]\n",
    "    # we should fit the one has larger value set\n",
    "\n",
    "    if (len(set(featureListTrain)) > len(set(featureListTest))):\n",
    "        \n",
    "        \n",
    "        enc.fit(featureListTrain) \n",
    "    else:\n",
    "        enc.fit(featureListTest)\n",
    "    \n",
    "    labelEncodedFeatureTrain = enc.transform(featureListTrain).reshape(-1, 1)\n",
    "    labelEncodedFeatureTest = enc.transform(featureListTest).reshape(-1, 1)\n",
    "    \n",
    "    # oneHot encoding\n",
    "    enc = OneHotEncoder()\n",
    "    if (len(set(featureListTrain)) > len(set(featureListTest))):\n",
    "        enc.fit(labelEncodedFeatureTrain) # use train to fit the data\n",
    "    else:\n",
    "        enc.fit(labelEncodedFeatureTest)\n",
    "    \n",
    "    # return order: train, test\n",
    "    return enc.transform(labelEncodedFeatureTrain).toarray(), enc.transform(labelEncodedFeatureTest).toarray()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dayMonthYearFeatureOneHotKeyEncoder(feature_column_vector_train, feature_column_vector_test, featureName):\n",
    "    if (featureName == 'Day'):\n",
    "        featureList = list(range(1, 32))\n",
    "    elif (featureName == 'Month'):\n",
    "        featureList = list(range(1, 13))\n",
    "    else:\n",
    "        featureList = list(range(2013, 2017))\n",
    "        \n",
    "    enc = LabelEncoder()\n",
    "    enc.fit(featureList)\n",
    "    \n",
    "    featureListTrain = (feature_column_vector_train).tolist()\n",
    "    featureListTest = (feature_column_vector_test).tolist()\n",
    "    \n",
    "    labelEncodedFeatureTrain = enc.transform(featureListTrain).reshape(-1, 1)\n",
    "    labelEncodedFeatureTest = enc.transform(featureListTest).reshape(-1, 1)\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(labelEncodedFeatureTrain)\n",
    "    \n",
    "    return enc.transform(labelEncodedFeatureTrain).toarray(), enc.transform(labelEncodedFeatureTest).toarray()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numericalTrainTransformation(headerTrain, dataTrain, headerTest, dataTest):\n",
    "    # This function prepares from the clean data to all numerical numpy array ready to be feed into DMatrix\n",
    "    # We will finally safely remove the store index column\n",
    "    \n",
    "    headerNew = [ \"DayOfWeek1\", \"DayOfWeek2\", \"DayOfWeek3\", \"DayOfWeek4\", \"DayOfWeek5\", \"DayOfWeek6\", \n",
    "                  \"DayOfWeek7\",\n",
    "                  \"Number of Customers\", \n",
    "                  \"Open -dummy -True by default\",\n",
    "                  \"PromoBoolean\",\"PromoBoolean2\",\n",
    "                  \"StateHoliday1\", \"StateHoliday2\" ,\"StateHoliday3\",\n",
    "                  \"SchoolHolidayBoolean\", \"SchoolHolidayBoolean\",\n",
    "                  \"Month In Promotion 1\", \"Month In Promotion 2\",\n",
    "                  \"Store type 1\", \"store type 2\", \"store type 3\", \"store type 4\",\n",
    "                  \"assortmentType1\", \"assortmentType2\", \"assortmentType3\",\n",
    "                  \"Competition distance reciprocal\", \"competitionSinceDayCount\",\n",
    "                  \"promotionSinceDayCount\", \"promotionInterval1\", \"promotionInterval2\",\n",
    "                  \"promotionInterval3\", \"promotionInterval4\",\n",
    "                  'Average Sales',\n",
    "                  'Variance Sales',\n",
    "                  'Average SC Ratio',\n",
    "                  'Variance SC Ratio',\n",
    "                  'Median Sales',\n",
    "                 'Average Open Ratio'\n",
    "                ]\n",
    "        \n",
    "    dayOfWeekColumnTrain, dayOfWeekColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,0], dataTest[:,0])\n",
    "    customersNumberColumnTrain, customersNumberColumnTest = dataTrain[:,1], dataTest[:,1]\n",
    "    \n",
    "    openBooleanTrain, openBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,2], dataTest[:,2]) \n",
    "    promoBooleanTrain, promoBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,3], dataTest[:,3]) \n",
    "    stateHolidayBooleanTrain, stateHolidayBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,4], dataTest[:,4])\n",
    "    schoolHolidayBooleanTrain, schoolHolidayBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,5], dataTest[:,5])\n",
    "    \n",
    "    monthInPromotionBooleanTrain, monthInPromotionBooleanTest = singleFeatureOneHotKeyEncoder(dataTrain[:,9], dataTest[:,9])\n",
    "    storeTypeColumnTrain, storeTypeColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,10], dataTest[:,10])\n",
    "    assortmentTypeColumnTrain, assortmentTypeColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,11], dataTest[:,11])\n",
    "    \n",
    "    competitionDistanceReciprocalTrain, competitionDistanceReciprocalTest = dataTrain[:,12], dataTest[:,12]\n",
    "    competitionSinceDayCountTrain, competitionSinceDayCountTest = dataTrain[:,13], dataTest[:,13]\n",
    "    promotionSinceDayCountTrain, promotionSinceDayCountTest = dataTrain[:,14], dataTest[:, 14]\n",
    "    \n",
    "    promotionIntervalColumnTrain, promotionIntervalColumnTest = singleFeatureOneHotKeyEncoder(dataTrain[:,15], dataTest[:,15])\n",
    "    \n",
    "    calculatedStatisticsColumnTrain, calculatedStatisticsColumnTest = dataTrain[:,16:], dataTest[:,16:]\n",
    "    \n",
    "    # use numpy.column_stack to accomplish column and matrix side by side stacking\n",
    "    resultantArrayTrain = np.column_stack((dayOfWeekColumnTrain,                                      \n",
    "                                      customersNumberColumnTrain,\n",
    "                                      openBooleanTrain,\n",
    "                                      promoBooleanTrain,\n",
    "                                      stateHolidayBooleanTrain,\n",
    "                                      schoolHolidayBooleanTrain,\n",
    "                                      monthInPromotionBooleanTrain,\n",
    "                                      storeTypeColumnTrain,\n",
    "                                      assortmentTypeColumnTrain,\n",
    "                                      competitionDistanceReciprocalTrain,\n",
    "                                      competitionSinceDayCountTrain,\n",
    "                                      promotionSinceDayCountTrain,\n",
    "                                      promotionIntervalColumnTrain,\n",
    "                                      calculatedStatisticsColumnTrain))\n",
    "    \n",
    "    resultantArrayTest = np.column_stack((dayOfWeekColumnTest,                                      \n",
    "                                      customersNumberColumnTest,\n",
    "                                      openBooleanTest,\n",
    "                                      promoBooleanTest,\n",
    "                                      stateHolidayBooleanTest,\n",
    "                                      schoolHolidayBooleanTest,\n",
    "                                      monthInPromotionBooleanTest,\n",
    "                                      storeTypeColumnTest,\n",
    "                                      assortmentTypeColumnTest,\n",
    "                                      competitionDistanceReciprocalTest,\n",
    "                                      competitionSinceDayCountTest,\n",
    "                                      promotionSinceDayCountTest,\n",
    "                                      promotionIntervalColumnTest,\n",
    "                                      calculatedStatisticsColumnTest))\n",
    "    \n",
    "    return resultantArrayTrain, resultantArrayTest, headerNew # order: train, test, headerNew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the full infomation (contains categorical information) we have into numerical numpy array that have each entry radily converted to floating numbers using one hot key encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataTrainNumerical, dataTestNumerical, headerWhole = \\\n",
    "    numericalTrainTransformation(headerTrainClean, dataTrainClean, \n",
    "                                 headerTestClean, dataTestClean)\n",
    "dataTrainNumericalLog, dataTestNumericalLog, headerWholeLog= \\\n",
    "    numericalTrainTransformation(headerTrainCleanLog, dataTrainCleanLog, \n",
    "                                 headerTestCleanLog, dataTestCleanLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for restore real prediction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restoreExponential(predictionWithZero):\n",
    "    # restore the logged prediction values\n",
    "    oriList = predictionWithZero.tolist()\n",
    "    resultant_list = []\n",
    "    for i in oriList:\n",
    "        resultant_list.append(math.exp(i))\n",
    "    return np.array(resultant_list)\n",
    "\n",
    "def restoreSalesFromRatio(data, prediction, customerIndex):\n",
    "    resultant_list = []\n",
    "    N = len(prediction.tolist())\n",
    "    for i in range(N):\n",
    "        resultant_list.append(prediction[i] * data[i][customerIndex])\n",
    "    return np.array(resultant_list)\n",
    "\n",
    "def restoreZeroEntryInPrediction(prediction, dataTestCleanWithZero, openBooleanIndex):\n",
    "    # restore the zero entries in the prediction\n",
    "    resultantList= []\n",
    "    predictionIndex = 0\n",
    "    for i in range(dataTestCleanWithZero.shape[0]):\n",
    "        if (dataTestCleanWithZero[i][openBooleanIndex] is True): # the store is open, put our prediction inside\n",
    "            resultantList.append(prediction[predictionIndex])\n",
    "            predictionIndex += 1 # update prediction index to the next prediction point\n",
    "        else:\n",
    "            resultantList.append(0) # the store is closed, append zero and do not update the index\n",
    "    return np.array(resultantList).reshape(len(resultantList),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model - Initial Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def XGBoostTrain(dataTrain, dataTest, labelTrain, params, ratioBoolean, logBoolean, customerIndex, \n",
    "                 dataTestRawZero, openBooleanIndex, adjustment):\n",
    "    dTrain = xgb.DMatrix(dataTrain, label = labelTrain)\n",
    "    dTest = xgb.DMatrix(dataTest)\n",
    "    bst = xgb.train(params=params, dtrain=dTrain)\n",
    "    prediction = bst.predict(dTest) \n",
    "    \n",
    "    if (ratioBoolean):\n",
    "        prediction = restoreSalesFromRatio(dataTest, prediction, customerIndex)\n",
    "    \n",
    "    if (logBoolean):\n",
    "        prediction = restoreExponential(prediction)\n",
    "        \n",
    "    prediction = restoreZeroEntryInPrediction(prediction, dataTestRawZero, openBooleanIndex)\n",
    "    \n",
    "    return bst, prediction * adjustment # order: booster, prediction numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost self-defined objective functions and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalError(prediction, label, data, ratio, customerIndex, log):    \n",
    "    N = label.shape[0]\n",
    "    labelShould = label\n",
    "    error = 0\n",
    "    if ratio is True:\n",
    "        prediction = restoreSalesFromRatio(data, prediction, customerIndex)\n",
    "        labelShould = restoreSalesFromRatio(data, label, customerIndex)\n",
    "        \n",
    "    if log is True:\n",
    "        prediction = restoreExponential(prediction)\n",
    "        labelShould = restoreExponential(label)\n",
    "        \n",
    "    for i in range (N):\n",
    "        t = labelShould[i]\n",
    "        p = prediction[i] \n",
    "        error += ((p-t)/t)**2\n",
    "    return float((error/N)**(1/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation time functions for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgbcv_helper(trainingSet, trainingLabel, validationSet, validationLabel, testingData, \n",
    "                 para, ratioLabelBoolean, customerIndex, logBoolean):\n",
    "    dTrain = xgb.DMatrix(trainingSet, label=trainingLabel)\n",
    "    dTest = xgb.DMatrix(testingData)\n",
    "    dValidation = xgb.DMatrix(validationSet)\n",
    "    dTrainWithoutLabel = xgb.DMatrix(trainingSet)\n",
    "    bst = xgb.train(params=para, dtrain=dTrain)\n",
    "    \n",
    "    prediction = bst.predict(dTest)\n",
    "\n",
    "    validationPrediction = bst.predict(dValidation)\n",
    "    trainPrediction = bst.predict(dTrainWithoutLabel)\n",
    "    \n",
    "    error = evalError(validationPrediction, validationLabel, validationSet, ratioLabelBoolean, customerIndex, logBoolean) \n",
    "    trainError = evalError(trainPrediction, trainingLabel, trainingSet, ratioLabelBoolean, customerIndex, logBoolean)\n",
    "    \n",
    "    #print(\"validation error\" + str(error))\n",
    "    #print(\"training error\" + str(trainError))\n",
    "    return prediction, bst, error, trainError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossValidationTimeSeries(dataTrainNumerical, label, kFold, dataTestNumerical, para, ratioLabelBoolean, customerIndex, logBoolean):\n",
    "    N = dataTrainNumerical.shape[0]\n",
    "    resultant_validation_error_list = []\n",
    "    resultant_training_error_list = []\n",
    "    k = int(math.floor(N/(kFold+1)))\n",
    "    # print(k)\n",
    "    \n",
    "    for multiplier in range(kFold, 0, -1): # the training data we have is in the reverse of time\n",
    "        \n",
    "        trainingSet = dataTrainNumerical[k*multiplier:,:]\n",
    "        validationSet = dataTrainNumerical[k*(multiplier-1):(k*multiplier), :]\n",
    "        trainingLabel = label[k*multiplier:]\n",
    "        validationLabel = label[k*(multiplier-1):(k*multiplier)]\n",
    "        \n",
    "        prediction , bst, validationError, trainingError= \\\n",
    "            xgbcv_helper(trainingSet, trainingLabel, validationSet, \n",
    "                         validationLabel, dataTestNumerical, para, ratioLabelBoolean, customerIndex, logBoolean)\n",
    "        \n",
    "        resultant_validation_error_list.append(validationError)\n",
    "        resultant_training_error_list.append(trainingError)\n",
    "    \n",
    "        print(validationError)\n",
    "                \n",
    "        #print(prediction)\n",
    "    print(float(sum(resultant_validation_error_list)/len(resultant_validation_error_list))) #average validation error\n",
    "    return resultant_validation_error_list, resultant_training_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10855846798451357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15771944832984305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16308717778126036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1251613819228103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08876697957766948\n0.12865869111921932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4784.61474609   6162.81689453   8055.07080078 ...,   7417.85302734\n  27684.65039062   6875.97412109]\n"
     ]
    }
   ],
   "source": [
    "# Full Model\n",
    "params = {      'eta'             : 0.4,\n",
    "                'nround'          : 5000,\n",
    "                'colsample_bytree': 0.9}\n",
    "\n",
    "\n",
    "validationError, trainingError = \\\n",
    "    crossValidationTimeSeries(dataTrainNumerical, labelTrain, 5, dataTestNumerical, params, False, 7, False)\n",
    "\n",
    "BST, prediction = XGBoostTrain(dataTrainNumerical, dataTestNumerical, labelTrain, params, False, False, 7,\n",
    "                                     dataRawTestZero, 4, 1)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13160641786265506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15618770090693332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08652256477668971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08181715049681244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07297608981729073\n0.10582198477207624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4230.90454317   5365.7039596    7764.18167981 ...,   7495.17108217\n  24243.16607153   6937.60091122]\n"
     ]
    }
   ],
   "source": [
    "# LogModel\n",
    "params = { \n",
    "                'eta'             : 0.4,\n",
    "                'nround'          : 3000,\n",
    "                'colsample_bytree': 0.9}\n",
    "    \n",
    "validationErrorLog, trainingErrorLog = \\\n",
    "    crossValidationTimeSeries(dataTrainNumericalLog, labelTrainLog, 5, dataTestNumericalLog, params, False, 7, True)\n",
    "\n",
    "BSTLog, predictionLog = XGBoostTrain(dataTrainNumericalLog, dataTestNumericalLog, labelTrainLog, params, False, True, 7,\n",
    "                                     dataRawTestZero, 4, 1)\n",
    "print(predictionLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement1 -- Process sales/customers ratio label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def salesCustomerRatioLabelConverter(dataTrain, labelTrain):\n",
    "    resultant_list = []\n",
    "    listing = labelTrain.tolist()\n",
    "    for i in range(len(listing)):\n",
    "        resultant_list.append( listing[i]/dataTrain[i][3] ) # sales / customers number\n",
    "    return np.array(resultant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelTrainRatio = salesCustomerRatioLabelConverter(dataTrain, labelTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07204626009894179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07713680322858207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08107674117795739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09116375863022896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06277184694499297\n0.07683908201614062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4904.64185715   5806.85913563   8158.87932682 ...,   7919.91522312\n  26627.46015549   7072.7364006 ]\n"
     ]
    }
   ],
   "source": [
    "# Ratio Full Model\n",
    "params = {      'eta'             : 0.4,\n",
    "                'nround'          : 5000,\n",
    "                'colsample_bytree': 0.9}\n",
    "\n",
    "\n",
    "validationError, trainingError = \\\n",
    "    crossValidationTimeSeries(dataTrainNumerical, labelTrainRatio, 5, dataTestNumerical, params, True, 7, False)\n",
    "\n",
    "BSTRatio, predictionRatio = XGBoostTrain(dataTrainNumerical, dataTestNumerical, labelTrainRatio, params, True, False, 7,\n",
    "                                     dataRawTestZero, 4, 1)\n",
    "\n",
    "print(predictionRatio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement2 -- Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importance(model, threshold): \n",
    "    impt = model.get_fscore()\n",
    "    index = []\n",
    "    for (key,value) in impt.items():\n",
    "        if impt[key] >= threshold: index.append(int(str(key)[1:]))\n",
    "    return index\n",
    "\n",
    "def select_features(index, dataTrain, dataTest):\n",
    "    partialTrain = np.column_stack((dataTrain[:,index[0]], dataTrain[:,index[1]]))\n",
    "    partialTest = np.column_stack((dataTest[:,index[0]], dataTest[:,index[1]]))\n",
    "    for j in range(2,len(index)):\n",
    "        i = index[j]\n",
    "        partialTrain = np.column_stack((partialTrain, dataTrain[:,i]))\n",
    "        partialTest = np.column_stack((partialTest, dataTest[:,i]))\n",
    "    return partialTrain, partialTest\n",
    "\n",
    "def getFeatureNames(headerWhole, indexList):\n",
    "    resultant_list = []\n",
    "    for i in indexList:\n",
    "        resultant_list.append(headerWhole[i])\n",
    "    return resultant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Average Sales', 'DayOfWeek6', 'DayOfWeek3', 'SchoolHolidayBoolean', 'DayOfWeek1', 'DayOfWeek2', 'Median Sales', 'Variance Sales', 'PromoBoolean', 'competitionSinceDayCount', 'Number of Customers', 'Average SC Ratio', 'promotionSinceDayCount', 'Variance SC Ratio', 'store type 4', 'Competition distance reciprocal', 'DayOfWeek7']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10919332586448115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17408069056165895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09480235859400928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0920912319723816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0687301272716593\n0.10777954685283805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4653.01611328   6029.45654297   8022.06884766 ...,   8037.51611328\n  25276.25976562   7168.25537109]\n"
     ]
    }
   ],
   "source": [
    "# Partial Full Model 1 (impt weight) \n",
    "index = get_importance(BST, 5)\n",
    "print(getFeatureNames(headerWhole, index))\n",
    "data1, test1 = select_features(index, dataTrainNumerical, dataTestNumerical)\n",
    "customerIndex = index.index(7)\n",
    "params1 = {     'eta'             : 0.3,\n",
    "                'nround'          : 3000,\n",
    "                'colsample_bytree': 0.8}\n",
    "validationError, trainingError = \\\n",
    "              crossValidationTimeSeries(data1, labelTrain, 5, test1, params1, False, customerIndex, False)\n",
    "          \n",
    "BST1, prediction1 = XGBoostTrain(data1, test1, labelTrain, params1, False, False, customerIndex,\n",
    "                                     dataRawTestZero, 4, 1)\n",
    "\n",
    "print(prediction1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Average Sales', 'Number of Customers', 'Average SC Ratio', 'assortmentType2', 'store type 4', 'DayOfWeek1', 'Average Open Ratio', 'SchoolHolidayBoolean', 'Variance SC Ratio', 'DayOfWeek6', 'Variance Sales', 'competitionSinceDayCount']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12313394032851228\n0.15322024877058923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07814800511560134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07791241413590734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07779984057187943\n0.10204288978449791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4290.07480247   5749.9156275    7803.39092388 ...,   7558.62608151\n  24335.77661076   6791.68845051]\n"
     ]
    }
   ],
   "source": [
    "# Partial Log Model 1 (impt weight) \n",
    "index = get_importance(BSTLog, 5)\n",
    "print(getFeatureNames(headerWhole, index))\n",
    "dataLog1, testLog1 = select_features(index, dataTrainNumericalLog, dataTestNumericalLog)\n",
    "customerIndex = index.index(7)\n",
    "params1 = {     'eta'             : 0.4,\n",
    "                'nround'          : 3000,\n",
    "                'colsample_bytree': 0.9}\n",
    "validationError, trainingError = \\\n",
    "              crossValidationTimeSeries(dataLog1, labelTrainLog, 5, testLog1, params1, False, customerIndex, True)\n",
    "          \n",
    "BSTLog1, predictionLog1 = XGBoostTrain(dataLog1, testLog1, labelTrainLog, params1, False, True, customerIndex,\n",
    "                                     dataRawTestZero, 4, 1)\n",
    "\n",
    "print(predictionLog1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Average SC Ratio', 'DayOfWeek6', 'DayOfWeek1', 'Number of Customers', 'SchoolHolidayBoolean', 'Month In Promotion 1', 'promotionInterval2', 'Average Sales', 'competitionSinceDayCount', 'Variance SC Ratio', 'promotionSinceDayCount', 'Variance Sales']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0740969813432794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0706297606771439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07583876065814896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09269642302531311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06234364383207458\n0.07512111390719198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4869.31973934   5945.98412514   8035.98097229 ...,   7924.94967079\n  26753.74673843   7059.88475418]\n"
     ]
    }
   ],
   "source": [
    "# Partial Ratio Model 1 (impt weight) \n",
    "index = get_importance(BSTRatio, 15)\n",
    "print(getFeatureNames(headerWhole, index))\n",
    "customerIndex = index.index(7)\n",
    "dataRatio1, testRatio1 = select_features(index, dataTrainNumerical, dataTestNumerical)\n",
    "params1 = {     'eta'             : 0.4,\n",
    "                'nround'          : 3000,\n",
    "                'colsample_bytree': 0.9}\n",
    "validationError, trainingError = \\\n",
    "                  crossValidationTimeSeries(dataRatio1, labelTrainRatio, 5, testRatio1, params1, True, customerIndex, False)  # kFold, customerIndex\n",
    "          \n",
    "BSTRatio1, predictionRatio1 = XGBoostTrain(dataRatio1, testRatio1, labelTrainRatio, params1, True, False, customerIndex,\n",
    "                                     dataRawTestZero, 4, 1) #customerIndex, openBooleanIndex, adjustment\n",
    "\n",
    "print(predictionRatio1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble on many models from sklearn ML framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Floating number converted training and testing data for SKLearn Regressors\n",
    "1. First Set: **full model** with trainData, testData, and label\n",
    "2. Second Set: **Log model** with trainDataLog, testDataLog, and labelLog\n",
    "3. Third Set: **Ratio model** with tranData, testData, and labelRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = dataTrainNumerical.astype(float)\n",
    "testData = dataTestNumerical.astype(float)\n",
    "trainDataLog = dataTrainNumericalLog.astype(float)\n",
    "testDataLog = dataTestNumericalLog.astype(float)\n",
    "labelLog = labelTrainLog.astype(float)\n",
    "label = labelTrain.astype(float)\n",
    "labelRatio = labelTrainRatio.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation for sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalErrorSKLearn(prediction, realLabel, data, customerIndex, logBoolean, ratioBoolean):\n",
    "    # Root Mean Square Percentage Error (RMSPE), return a floating number\n",
    "    N = realLabel.shape[0]\n",
    "    realLabel = realLabel.copy()\n",
    "    prediction = prediction.copy()\n",
    "    \n",
    "    if (ratioBoolean):\n",
    "        prediction = restoreSalesFromRatio(data, prediction, customerIndex)\n",
    "        realLabel = restoreSalesFromRatio(data, realLabel, customerIndex)\n",
    "    \n",
    "    if (logBoolean):\n",
    "        prediction = restoreExponential(prediction)\n",
    "        realLabel = restoreExponential(realLabel)\n",
    "    \n",
    "    error = 0\n",
    "    for i in range (N):\n",
    "        t = realLabel[i]\n",
    "        p = prediction[i]\n",
    "        error += ((p-t)/t)**2\n",
    "    return float((error/N)**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossValidationSKLearnLinearModel(trainData, label, testData, kFold, customerIndex, logBoolean, ratioBoolean, SKLearn, **para):\n",
    "    # para is the optional parameters we want to pass in to respective SKLearn linear models\n",
    "    # SKLearn is the linear model we want to test out\n",
    "    N = trainData.shape[0]\n",
    "    resultant_validation_error_list = []\n",
    "    resultant_training_error_list = []\n",
    "    k = int(math.floor(N/(kFold+1)))\n",
    "    # print(k)\n",
    "    \n",
    "    for multiplier in range(kFold, 0, -1): # the training data we have is in the reverse of time\n",
    "        \n",
    "        trainingSet = trainData[k*multiplier:,:]\n",
    "        validationSet = trainData[k*(multiplier-1):(k*multiplier), :]\n",
    "        trainingLabel = label[k*multiplier:]\n",
    "        validationLabel = label[k*(multiplier-1):(k*multiplier)]\n",
    "        \n",
    "        # modelling and prediction\n",
    "        model = SKLearn(**para)\n",
    "        model.fit(X = trainingSet, y = trainingLabel)\n",
    "        validationPrediction = model.predict(X = validationSet)\n",
    "        trainingPrediction = model.predict(X = trainingSet)\n",
    "        \n",
    "        # error calculation\n",
    "        validationError = evalErrorSKLearn(validationPrediction,validationLabel, validationSet, customerIndex, logBoolean, ratioBoolean)\n",
    "        trainingError = evalErrorSKLearn(trainingPrediction, trainingLabel, trainingSet, customerIndex, logBoolean, ratioBoolean)\n",
    "        \n",
    "        resultant_validation_error_list.append(validationError)\n",
    "        resultant_training_error_list.append(trainingError)\n",
    "        print(validationError)\n",
    "    # final validation error display\n",
    "    print(float(sum(resultant_validation_error_list)/len(resultant_validation_error_list))) #average validation error\n",
    "    return resultant_validation_error_list, resultant_training_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SKLearnTrain(trainData, label, testData, customerIndex, logBoolean, ratioBoolean, SKLearn, **para):\n",
    "    model = SKLearn(**para)\n",
    "    model.fit(X = trainData, y = label)\n",
    "    prediction = model.predict(X = testData)\n",
    "    \n",
    "    if (ratioBoolean):\n",
    "        prediction = restoreSalesFromRatio(testData, prediction, customerIndex)\n",
    "    if (logBoolean):\n",
    "        prediction = restoreExponential(prediction)\n",
    "        \n",
    "    res = restoreZeroEntryInPrediction(prediction, dataRawTestZero, 4)\n",
    "    print (res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20611918432699905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4532808464048451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29678114996041693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2834751023539924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25729666832274384\n0.2993905902737995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5894.69837353   6281.20327028   7339.84432824 ...,   8119.60751809\n  19950.53459821   6281.20327028]\n"
     ]
    }
   ],
   "source": [
    "# Adaboost: full model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, label, testData, 5, customerIndex, False, False, ensemble.AdaBoostRegressor, \n",
    "                                      n_estimators=5, loss='exponential', learning_rate=0.3)\n",
    "prediction_ada = SKLearnTrain(trainData, label, testData, customerIndex, False, False, ensemble.AdaBoostRegressor,\n",
    "             n_estimators=5, loss='exponential', learning_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16651832076980483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34956501454106426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1892649418431169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20576049539423064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17230345030287278\n0.21668244457021793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5191.31293646   6467.59714424   9122.38335082 ...,   9122.38335082\n  17866.74677533   6104.04507367]\n"
     ]
    }
   ],
   "source": [
    "# Adaboost: Log model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainDataLog, labelLog, testDataLog, 5, customerIndex, True, False, ensemble.AdaBoostRegressor, \n",
    "                                      n_estimators=15, loss='exponential', learning_rate=0.5)\n",
    "prediction_ada_log = SKLearnTrain(trainDataLog, labelLog, testDataLog, customerIndex, True, False, ensemble.AdaBoostRegressor,\n",
    "             n_estimators=15, loss='exponential', learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0863121353548269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0862713397418665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09523145361947849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12130637026418474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08769544887093352\n0.09536334957025802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4916.03482099   6174.07354052   8110.26300283 ...,   7872.33389494\n  29259.72222825   6832.3496712 ]\n"
     ]
    }
   ],
   "source": [
    "# Adaboost: ratio model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, labelRatio, testData, 5, customerIndex, False, True, ensemble.AdaBoostRegressor, \n",
    "                                      n_estimators=5, loss='exponential', learning_rate=0.3)\n",
    "prediction_ada_ratio = SKLearnTrain(trainData, labelRatio, testData, customerIndex, False, True, ensemble.AdaBoostRegressor,\n",
    "             n_estimators=5, loss='exponential', learning_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09925332502374767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16035569906097322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08763447486268866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09740379019587135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06695210733045313\n0.1023198792947468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4685.5   5855.7   8374.6 ...,   7860.   26129.    7242.7]\n"
     ]
    }
   ],
   "source": [
    "# Bagging: full model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, label, testData, 5, customerIndex, False, False, ensemble.BaggingRegressor,\n",
    "                                      n_estimators = 10, max_samples = 1.0, max_features = 1.0)\n",
    "prediction_bag = SKLearnTrain(trainData, label, testData, customerIndex, False, False, ensemble.BaggingRegressor,\n",
    "                                      n_estimators = 10, max_samples = 1.0, max_features = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09547363398243891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1722354370652777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08087526460904677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09354097093117539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06634260258057953\n0.10169358183370365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4653.39818695   5910.96483541   8237.4369925  ...,   7771.22625527\n  26808.70533924   7200.69869278]\n"
     ]
    }
   ],
   "source": [
    "# Bagging: log model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainDataLog, labelLog, testDataLog, 5, customerIndex, True, False, ensemble.BaggingRegressor,\n",
    "                                      n_estimators = 10, max_samples = 1.0, max_features = 1.0)\n",
    "prediction_bag_log = SKLearnTrain(trainDataLog, labelLog, testDataLog, customerIndex, True, False, ensemble.BaggingRegressor,\n",
    "                                      n_estimators = 10, max_samples = 1.0, max_features = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07631364694503094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08447573929968384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08826459510922612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08765554835097811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06168818229851206\n0.07967954240068623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4472.52458497   5859.47516433   8239.5790591  ...,   7818.85315801\n  27524.19437055   7040.5601102 ]\n"
     ]
    }
   ],
   "source": [
    "# Bagging: ratio model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, labelRatio, testData, 5, customerIndex, False, True, ensemble.BaggingRegressor,\n",
    "                                      n_estimators = 10, max_samples = 1.0, max_features = 1.0)\n",
    "prediction_bag_ratio = SKLearnTrain(trainData, labelRatio, testData, customerIndex, False, True, ensemble.BaggingRegressor,\n",
    "                                      n_estimators = 10, max_samples = 1.0, max_features = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13349437749426282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19566867859546405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1314135103438069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10623794487677782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0716605164697768\n0.12769500555601768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4792.4   5871.6   7956.5 ...,   7918.2  27182.1   7112. ]\n"
     ]
    }
   ],
   "source": [
    "# Extra tree: full model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, label, testData, 5, customerIndex, False, False, ensemble.ExtraTreesRegressor)\n",
    "prediction_extra_tree_full = SKLearnTrain(trainData, label, testData, customerIndex, False, False, ensemble.ExtraTreesRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12935552264770772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20672107808992174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12860946871351578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0957784375197803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07096606032431153\n0.12628611345904742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4635.45132475   5680.58931641   8146.80996433 ...,   7836.08303549\n  26184.50795031   6899.00608788]\n"
     ]
    }
   ],
   "source": [
    "# Extra tree: log model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainDataLog, labelLog, testDataLog, 5, customerIndex, True, False, ensemble.ExtraTreesRegressor)\n",
    "prediction_extra_tree_log = SKLearnTrain(trainDataLog, labelLog, testDataLog, customerIndex, True, False, ensemble.ExtraTreesRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07744912596749193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09046815125190233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09066996333251862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0885399315267407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07273995627516087\n0.08397342567076288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4476.1899007    5807.30360493   8209.41653534 ...,   7640.24730873\n  26896.23625812   7079.46070281]\n"
     ]
    }
   ],
   "source": [
    "# Extra tree: ratio model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, labelRatio, testData, 5, customerIndex, False, True, ensemble.ExtraTreesRegressor)\n",
    "prediction_extra_tree_ratio = SKLearnTrain(trainData, labelRatio, testData, customerIndex, False, True, ensemble.ExtraTreesRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12007281254546166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17447584286555293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12233600676928329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12679875057625592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08152628388929017\n0.1250419393291688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5108.47284209   6044.74513063   8299.64649567 ...,   7820.79791977\n  26800.79594919   7227.27025411]\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting: full model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, label, testData, 5, customerIndex, False, False, ensemble.GradientBoostingRegressor)\n",
    "prediction_gradient_full = SKLearnTrain(trainData, label, testData, customerIndex, False, False, ensemble.GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11106235529463163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15167920677424634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10309751390413727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10587190679066066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07030452984772423\n0.10840310252228003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4914.90174413   6049.41015053   8125.60681771 ...,   7943.69310421\n  24081.22219837   7211.87158352]\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting: log model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainDataLog, labelLog, testDataLog, 5, customerIndex, True, False, ensemble.GradientBoostingRegressor)\n",
    "prediction_gradient_log = SKLearnTrain(trainDataLog, labelLog, testDataLog, customerIndex, True, False, ensemble.GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07337479796785006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07313464689581245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08338624789595053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09786193687383962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0618487180114386\n0.07792126952897825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4867.64871466   5896.18750165   8099.15648636 ...,   7970.13976856\n  26813.06436348   7129.65296716]\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting: ratio model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, labelRatio, testData, 5, customerIndex, False, True, ensemble.GradientBoostingRegressor)\n",
    "prediction_gradient_ratio = SKLearnTrain(trainData, labelRatio, testData, customerIndex, False, True, ensemble.GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09600000266666228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16141633114043108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08529004484610525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09545167044074772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0658315576977503\n0.10079792135833934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4850.33333333   5798.13333333   8052.46666667 ...,   7884.6\n  27874.06666667   7099.33333333]\n"
     ]
    }
   ],
   "source": [
    "# Random forest: full model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, label, testData, 5, customerIndex, False, False, ensemble.RandomForestRegressor,\n",
    "                                      n_estimators=15, max_features = 'auto')\n",
    "prediction_random_full = SKLearnTrain(trainData, label, testData, customerIndex, False, False, ensemble.RandomForestRegressor,\n",
    "                                      n_estimators=15, max_features = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09637982186592346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1711013617282746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08091349031448862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09306220412898998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0654055684462339\n0.10137248929678211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4626.82496999   5857.55391777   8474.39576136 ...,   7681.57798245\n  27380.25590932   7210.7843548 ]\n"
     ]
    }
   ],
   "source": [
    "# Random forest: log model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainDataLog, labelLog, testDataLog, 5, customerIndex, True, False, ensemble.RandomForestRegressor,\n",
    "                                      n_estimators=15, max_features = 'auto')\n",
    "prediction_random_log = SKLearnTrain(trainDataLog, labelLog, testDataLog, customerIndex, True, False, ensemble.RandomForestRegressor,\n",
    "                                      n_estimators=15, max_features = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07491790625380754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.083945260512553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0869746182275697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08611672662939417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.060674665279342226\n0.07852583538053333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4650.45532302   5879.84487318   8187.80560839 ...,   7753.4398963\n  27392.52799173   7113.16373764]\n"
     ]
    }
   ],
   "source": [
    "# Random forest: ratio model\n",
    "customerIndex = 7\n",
    "validationList, trainingList = \\\n",
    "    crossValidationSKLearnLinearModel(trainData, labelRatio, testData, 5, customerIndex, False, True, ensemble.RandomForestRegressor,\n",
    "                                      n_estimators=15, max_features = 'auto')\n",
    "prediction_random_ratio = SKLearnTrain(trainData, labelRatio, testData, customerIndex, False, True, ensemble.RandomForestRegressor,\n",
    "                                      n_estimators=15, max_features = 'auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arithmeticAverage(* predictions):\n",
    "    N = predictions[0].shape[0]\n",
    "    numberOfPrediction = len(predictions)\n",
    "    resultantList = []\n",
    "    for i in range(N):\n",
    "        summation = 0\n",
    "        for prediction in predictions:\n",
    "            summation += prediction[i]\n",
    "        resultantList.append(summation/numberOfPrediction)\n",
    "    return np.array(resultantList)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_final = arithmeticAverage(predictionRatio, predictionRatio1, predictionLog, predictionLog1, prediction1,\n",
    "                                     prediction_ada_ratio, prediction_bag, prediction_bag_log, prediction_bag_ratio,\n",
    "                                     prediction_extra_tree_ratio, prediction_gradient_log, prediction_gradient_ratio,\n",
    "                                     prediction_random_full, prediction_random_log, prediction_random_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = os.path.join(directory_path, 'FinalPrediction.csv')\n",
    "writeToFile(prediction_final, filePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
